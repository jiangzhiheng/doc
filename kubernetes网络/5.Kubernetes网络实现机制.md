一、`Kubernetes Service`官方实现

1. 概述
   - 当用户创建`Service`和对应的后端`Pod`时，`Endpoints Controller`会监控`Pod`的状态变化，当`Pod`处于`Running`且准备就绪状态时，`Endpoints Controller`会生成`Endpoints`对象
   - 运行在每个节点上的`Kube-proxy`会监控`Service`和`Endpoint`的更新，并调用其`LoadBalancer`模块在主机上刷新路由转发规则
   - `Kube-proxy`的`LoadBalancer`模块实现有`userspace,iptables,IPVS`三种，当前主流的实现方式是`iptables和IPVS`
   - `Kube-proxy`的转发模式可以通过启动参数`--proxy-mode`进行配置，有`userspace,iptables,ipvs`等选项。
   
2. `userspace`模式

3. `iptables`模式
   - `Kube-proxy`针对于`NodePort`流量入口专门创建了`KUBE-NODEPORTS`链，由`KUBE-NODEPORTS`链跳转到`KUBE-SVC-*`链
   - 对于`ClusterIP`访问方式，`KUBE-SERVICES`链是访问集群内服务的数据包入口点，它会根据匹配到的目标`IP:port`将数据包分发到对应的`KUBE-SVC-*`链
   - `KUBE-SVC-*`相当于一个负载均衡器，利用了`iptables`的`random`模块，它会将数据包平均分发到到`KUBE-SEP-*`链。每个`KUBE-SVC-*`后面的`KUBE-SEP-*`链都和`Service`后端的`Pod`数量一样
   - `KUBE-SEP-*`链通过`DNAT`将连接的目的地址和端口从`Service`的`IP:port`替换为后端`Pod`的`IP:port`，从而将流量转发到相应的`Pod`
   - 为了保证回程报文能够顺利返回，需要在网关处做一次`SNAT`把报文的源`IP`修改成网关`IP`地址。
   
4. `IPVS`模式

   1. `IPVS`工作原理

      - `DR`模式
      - `Tunneling`模式
      - `NAT`模式：`Kubernetes`在用`IPVS`实现`Service`时使用的就是此模式

   2. `kube-proxy IPVS`模式参数（如果是`Kubeadm`部署的集群，则可通过修改`configmap`实现修改参数）

      - `--proxy-mode`：`IPVS`模式通过`--proxy-mode=ipvs`启用，默认是`iptables`
      - `--ipvs-scheduler`：用来指定`IPVS`负载均衡算法
        - `rr`
        - `lc`：最少连接数
        - `dh`：目的地址哈希
        - `sh`：源地址哈希
        - `sed`：最短延时
      - `--cleanup-ipvs`：如果设置为`true`，则清除在`IPVS`模式下创建的`IPVS`规则
      - `--ipvs-sync-period`：表示`kube-proxy`刷新`IPVS`规则的最大时间间隔
      - `--ipvs-min-sync-period`
      - `ipvs-exclude-cidrs`

      目前，本地`local-up`脚本，`GCE`安装脚本，`kubeadm`都支持通过到处环境变量(`KUBE_PROXY_MODE=ipvs`)切换到`IPVS`模式。

   3. `IPVS`模式实现原理

      - 确保一块`dummy`网卡(`kube-ipvs0`)存在，因为`IPVS`的`netfilter`钩子挂在`INPUT`链，我们需要把`Service`的访问`IP`绑定在`dummy`网卡上让那个觉得虚拟`IP`就是本机`IP`，进而进入`INPUT`链
      - 把`Service`的访问`IP`绑定在`dummy`网卡上。
      - 通过`Socket`调用，创建`IPVS`的`virtual server`和`real server`，分别对应`Kubernetes`的`Service`和`Endpoints`

5. `conntrack`

   `conntrack`中4个主要的状态

   - `NEW`
   - `ESTABLISHED`
   - `RELATED`
   - `INVALID`

二、`Ingress Controller`

1. `Ingress Controller`的通用框架

   - `Ingress Controller`实质上可以理解为监视器，`Ingress Controller`通过不断的跟`Kubernetes API`打交道，实时的感知后端`Service，Pod`等的变化；
   - 当得到变化信息后，`Ingress Controller`再结合下文的`Ingress`生成配置，然后更新反向代理负载均衡器，并刷新其配置，起到服务发现的作用。
   - `Ingress Controller`将`Ingress`入口地址和后端`Pod`地址的映射关系实时刷新到`Load Balancer`的配置文件中，再让负载均衡器`reload`该规则，便可实现服务的负载均衡和自动发现
2. `Nginx Ingress Controller`详解
   1. 在`Kubernetes`中，`Ingress Controller`将以`Pod`的形式运行，监控`API Server`的`/ingress`接口，后端的`backend services`，如果`Service`发生变化，则`Ingress Controller`应自动更新其转发规则
   2. 对于一个使用`Nginx`实现的`Ingress Controller`，需要实现的基本逻辑如下，
      - 监听`API Server`，获取全部`Ingress`定义
      - 基于`Ingress`定义，生成`nginx`所需的配置文件`/etc/nginx/nginx.conf`
      - 执行`nginx -s reload`命令，重新加载配置文件的内容
   3. 

三、`Kubernetes DNS`架构

1. `kube-dns`工作原理
   - `etcd+kube2sky+SkyDNS`
     - `etcd`：存储所有`DNS`查询需要的数据
     - `kube2sky`：观察`API Server`处`Service`和`Endpoints`的变化，然后同步到`etcd`
     - `SkyDNS`：监听53端口，根据`etcd`中的数据提供查询服务
   - `kubedns+dnsmasq+exechealthz`
     - `kubedns`：观察`API Server`处`Service`和`Endpoints`的变化，调用`SkyDNS`的`golang`库，在内存中维护`DNS`记录
     - `dnsmasq`：`DNS`配置工具，监听53端口，为集群提供`DNS`查询服务，提供缓存功能
     - `exechealthz`：健康检查
2. `CoreDNS`
   - 插件化
   
     常见插件有：
   
     - `loadbalance`：提供基于`DNS`的负载均衡功能
     - `loop`：检测在`DNS`解析过程中出现的简单循环问题
     - `cache`：提供前端缓存功能
     - `health`：对`endpoint`进行健康检查
     - `kubernetes`：从`kubernetes`中读取`zone`数据，可以用于自定义域名记录
     - `etcd`：从`etcd`读取`zone`数据，可以用于自定义域名记录
     - `file`：从`RFC1035`格式文件中读取`zone`数据
     - `hosts`：使用`/etc/hosts`文件或其它文件读取`zone`数据，可以用于自定义域名记录
     - `auto`：从磁盘中自动加载区域文件
     - `reload`：定时自动重新加载`Corefile`配置文件的内容
     - `forward`：转发域名查询到上游`DNS`服务器
     - `proxy`：转发特定的域名查询到多个其它`DNS`服务器，同时提供到多个`DNS`服务器负载均衡的功能
     - `prometheus`：为`Prometheus`系统提供采集性能指标数据的`URL`
     - `pprof`：在`URL`路径`/debug/pprof`下提供运行时的性能数据
     - `log`：对`DNS`查询记录进行日志记录
     - `errors`：对错误信息进行日志记录
   
   - 配置简单化：引入表达力更强的`DSL`，即`Corefile`形式的配置文件。
   
     示例：
   
     ```yaml
     [root@master ~]# kubectl get cm coredns -o yaml -n kube-system
     apiVersion: v1
     data:
       Corefile: |
         .:53 {
             errors
             health {
                lameduck 5s
             }
             ready
             kubernetes cluster.local in-addr.arpa ip6.arpa {
                pods insecure
                fallthrough in-addr.arpa ip6.arpa
                ttl 30
             }
             prometheus :9153
             forward . /etc/resolv.conf
             cache 30
             loop
             reload
             loadbalance
         }
     kind: ConfigMap
     metadata:
       creationTimestamp: "2020-03-13T13:14:52Z"
       name: coredns
       namespace: kube-system
       resourceVersion: "176"
       selfLink: /api/v1/namespaces/kube-system/configmaps/coredns
       uid: 1e71ead1-12ae-4565-a5f4-d64e2bb5d994
     
     ```
   
   - 一体化的解决方案

四、使用`Calico`提供`Kubernetes`网络策略

