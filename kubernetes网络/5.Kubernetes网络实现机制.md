一、`Kubernetes Service`官方实现

1. 概述
   - 当用户创建`Service`和对应的后端`Pod`时，`Endpoints Controller`会监控`Pod`的状态变化，当`Pod`处于`Running`且准备就绪状态时，`Endpoints Controller`会生成`Endpoints`对象
   - 运行在每个节点上的`Kube-proxy`会监控`Service`和`Endpoint`的更新，并调用其`LoadBalancer`模块在主机上刷新路由转发规则
   - `Kube-proxy`的`LoadBalancer`模块实现有`userspace,iptables,IPVS`三种，当前主流的实现方式是`iptables和IPVS`
   - `Kube-proxy`的转发模式可以通过启动参数`--proxy-mode`进行配置，有`userspace,iptables,ipvs`等选项。
   
2. `userspace`模式

3. `iptables`模式
   - `Kube-proxy`针对于`NodePort`流量入口专门创建了`KUBE-NODEPORTS`链，由`KUBE-NODEPORTS`链跳转到`KUBE-SVC-*`链
   - 对于`ClusterIP`访问方式，`KUBE-SERVICES`链是访问集群内服务的数据包入口点，它会根据匹配到的目标`IP:port`将数据包分发到对应的`KUBE-SVC-*`链
   - `KUBE-SVC-*`相当于一个负载均衡器，利用了`iptables`的`random`模块，它会将数据包平均分发到到`KUBE-SEP-*`链。每个`KUBE-SVC-*`后面的`KUBE-SEP-*`链都和`Service`后端的`Pod`数量一样
   - `KUBE-SEP-*`链通过`DNAT`将连接的目的地址和端口从`Service`的`IP:port`替换为后端`Pod`的`IP:port`，从而将流量转发到相应的`Pod`
   - 为了保证回程报文能够顺利返回，需要在网关处做一次`SNAT`把报文的源`IP`修改成网关`IP`地址。
   
4. `IPVS`模式

   1. `IPVS`工作原理

      - `DR`模式
      - `Tunneling`模式
      - `NAT`模式：`Kubernetes`在用`IPVS`实现`Service`时使用的就是此模式

   2. `kube-proxy IPVS`模式参数（如果是`Kubeadm`部署的集群，则可通过修改`configmap`实现修改参数）

      - `--proxy-mode`：`IPVS`模式通过`--proxy-mode=ipvs`启用，默认是`iptables`
      - `--ipvs-scheduler`：用来指定`IPVS`负载均衡算法
        - `rr`
        - `lc`：最少连接数
        - `dh`：目的地址哈希
        - `sh`：源地址哈希
        - `sed`：最短延时
      - `--cleanup-ipvs`：如果设置为`true`，则清除在`IPVS`模式下创建的`IPVS`规则
      - `--ipvs-sync-period`：表示`kube-proxy`刷新`IPVS`规则的最大时间间隔
      - `--ipvs-min-sync-period`
      - `ipvs-exclude-cidrs`

      目前，本地`local-up`脚本，`GCE`安装脚本，`kubeadm`都支持通过到处环境变量(`KUBE_PROXY_MODE=ipvs`)切换到`IPVS`模式。

   3. `IPVS`模式实现原理

      - 确保一块`dummy`网卡(`kube-ipvs0`)存在，因为`IPVS`的`netfilter`钩子挂在`INPUT`链，我们需要把`Service`的访问`IP`绑定在`dummy`网卡上让那个觉得虚拟`IP`就是本机`IP`，进而进入`INPUT`链
      - 把`Service`的访问`IP`绑定在`dummy`网卡上。
      - 通过`Socket`调用，创建`IPVS`的`virtual server`和`real server`，分别对应`Kubernetes`的`Service`和`Endpoints`

5. `conntrack`

   `conntrack`中4个主要的状态

   - `NEW`
   - `ESTABLISHED`
   - `RELATED`
   - `INVALID`

二、`Ingress Controller`

1. `Ingress Controller`的通用框架

   - `Ingress Controller`实质上可以理解为监视器，`Ingress Controller`通过不断的跟`Kubernetes API`打交道，实时的感知后端`Service，Pod`等的变化；
   - 当得到变化信息后，`Ingress Controller`再结合下文的`Ingress`生成配置，然后更新反向代理负载均衡器，并刷新其配置，起到服务发现的作用。
   - `Ingress Controller`将`Ingress`入口地址和后端`Pod`地址的映射关系实时刷新到`Load Balancer`的配置文件中，再让负载均衡器`reload`该规则，便可实现服务的负载均衡和自动发现
2. `Nginx Ingress Controller`详解

三、`Kubernetes DNS`架构

1. `kube-dns`工作原理
   - `etcd+kube2sky+SkyDNS`
     - `etcd`：存储所有`DNS`查询需要的数据
     - `kube2sky`：观察`API Server`处`Service`和`Endpoints`的变化，然后同步到`etcd`
     - `SkyDNS`：监听53端口，根据`etcd`中的数据提供查询服务
   - `kubedns+dnsmasq+exechealthz`
     - `kubedns`：观察`API Server`处`Service`和`Endpoints`的变化，调用`SkyDNS`的`golang`库，在内存中维护`DNS`记录
     - `dnsmasq`：`DNS`配置工具，监听53端口，为集群提供`DNS`查询服务，提供缓存功能
     - `exechealthz`：健康检查
2. `CoreDNS`
   - 插件化
   - 配置简单化：引入表达力更强的`DSL`，即`Corefile`形式的配置文件。
   - 一体化的解决方案

四、使用`Calico`提供`Kubernetes`网络策略

