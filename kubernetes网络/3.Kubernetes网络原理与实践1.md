一、`Kubernetes`基本概念

1. 云原生：(`Cloud Native`)

   云原生技术有利于各组织在公有云，私有云和混合云等新型动态关井中，构建和运行可弹性扩展的应用。云原生的代表技术包括容器，服务网格，微服务，不可变基础设施和声明式`API`,这些技术能够构建容错性好，易于管理，便于观察的松耦合系统。结合可靠的自动化手段，云原生技术使工程师能够轻松的对系统做出频繁和可预测的重大改变

2. `Kubernetes`简介

   - `Pod`中的容器可以共享的资源有：
     - `PID`命名空间：`Pod`中的不同应用程序可以看到其他应用程序的进程`ID`(`k8s 1.8`之后取消了共享)
     - 网络命名空间：`Pod`中的多个容器能够访问同一个`IP`和端口范围
     - `IPC`命名空间：`Pod`中的多个容器能够使用`SystemV IPC`或`POSIX`消息队列进行通信
     - `UTS`命名空间：`Pod`中的多个容器共享一个主机名
     - `Volumes(共享存储卷)`：`Pod`中的各个容器可以访问在`Pod`级别定义的存储卷

3. `Kubernetes`实现的功能

   `Kubernetes`依赖其它项目提供完整的编排服务，要正确的实施`Kubernetes`可以集成云原生领域的其它开源项目，让用户可以管理自己的容器基础设施，例如

   - 仓库：`Harbor,Docker Registry`
   - 网络：`OpenvSwitch,CNI,Calico`
   - 监控：`Prometheus,Kibana,watchdog,Elastic`
   - 安全：`LDAP,SELinux,OAUTH,Spiffe`
   - 存储：`Rook,Torus`

二、`Kubernetes`网络

1. `k8s`网络模型需要解决的问题：

   - 各台服务器上的容器`IP`段不能重叠，所以需要某种`IP`段分配机制，为各台服务器分配独立的`IP`段
   - 从某个`Pod`发出的流量到达其所在服务器时，服务器的网络层应当具备根据目标`IP`地址，将流量转发到该`IP`所属`IP`段对应的目标服务器的能力
   - 总结起来，主要关注两方面：`IP`地址分配和路由

2. `k8s`网络基础

   1. `IP`地址分配
      - 节点网络
      - `pod`网络：用户可以在创建集群时通过`--pod-cidr`指定此范围
      - `ClusterIP`：集群网络，大部分情况下，集群网络不和节点网络在同一网段
   2. `Pod`出站流量
      - `Pod`到`Pod`：每个`Pod`都有自己的`IP`地址，运行在`Pod`内的应用都可以使用标准的端口号，不用重新映射到不同的随机端口号。所有的`Pod`之间都可以保持三层网络的连通性，比如可以`ping`通对方，相互发送数据包。
      - `Pod`到`Service`：为用户提供固定的访问端点，`Kubernetes`通过`Kube-proxy`实现这个功能，在每个节点都运行一个`Kube-proxy`进程，通过复杂的`iptables/IPVS`规则在`Pod`和`Service`之间进行各种过滤和`NAT`。
      - `Pod`到集群外：通过`SNAT`来处理。

3. `Kubernetes`架构综述

   1. 单`Pod`单`IP`模型
      - 每个`Pod`都有一个独立的`IP`，`pod`内所有容器共享一个`network namespace`。
      - 容器是一等公民，容器之间通信，不需要额外的`NAT`。
      - `Node`与容器网络直联，也不需要额外的`NAT`
   2. `k8s`创建一个`Pod`后，初始化网络协议栈的过程：
      - 当用户在`k8s`的`master`创建一个`pod`后，`kubelet`观察到新`pod`的创建，于是首先调用`CRI`创建`Pod`内的若干个容器
      - 创建第一个容器`pause`容器(具体逻辑是一启动就把自己阻塞在那里)，作用是占用一个`Linux`的`network namespace`。
      - `Pod`内的其它容器通过加入这个`network namespace`的方式共享同一个网络命名空间。
      - `Kubelet`目前支持两个网络驱动，分别是`Kubenet`和`CNI`，通过`CNI`实现容器的网络设备初始化工作，例如创建网络设备，例如`eth0`设备和分配`IP`地址。

4. `Kubernetes`主机内组网模型

   - `kubernetes`经典的主机内组网模型是`veth pair + bridge`方式
   - `kubernetes`使用`veth pair`将容器与主机的网络协议栈连接起来，容器放在主机根`network namespace`中`veth pair`的一端连接到`linux`网桥，可让同一节点上的各`Pod`之间相互通信。

5. `Kubernetes`跨节点组网模型

   `Kubernetes`跨接点组网解决方案主要有`bridge`和`overlay`两种：

   - 接在同一个网桥上的`Pod`通过局域网广播通信
   - `bridge`网络本身不解决容器的跨机通信问题，需要显式的书写主机路由表，映射目标容器网段和主机`IP`的关系，集群内如果有`N`个主机，需要`N-1`条路由表项
   - `overlay`网络，是构建在物理网络之上的一个虚拟网络，其中`VXLAN`是主流的`overlay`标准。`VXLAN`就是用`UDP`包头封装二层数据帧，即所谓的`MAC in UDP`

6. `Pod`的`hosts`文件

   在创建`pod`前，通过`pods.spec`字段的`hostAliases`字段为`pod`中的容器添加额外的`hosts`条目，通过`kubernetes`的`downward API`实现

   - `downward API`相关知识：`https://help.aliyun.com/document_detail/141788.html`

   示例：

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: hostaliases-pod
   spec:
     hostAliases:
     - ip: "127.0.0.1"
       hostnames:
       - "foo.local"
       - "bar.local"
     - ip: "10.1.1.3"
       hostnames:
       - "foo.remote"
       - "bar.remote"
     containers:
     - name: cat-hosts
       image: busybox
   ```

   注：如果`Pod`启用了`hostNetwork`（即使用主机网络），那么将不能使用`hostAliases`特性。

7. `Pod`的`hostname`

   1. 通过`UTS namespace`隔离技术实现`Pod`之间主机名相互隔离，但`Pod`内容器分享同一个主机名。
   2. 一个`Pod`内如果有多个容器，修改任意一个容器的`hostname`都会影响其它容器
   3. 如果通过`hostname`命令修改了主机名，容器重启或被`kubelet`重建都会使主机名恢复。

三、`Pod`的核心：`pause`容器

1. `Pause`容器的作用：
   - 在`Pod`中，它作为共享`Linux namespace`(`Network UTS`等)的基础
   - 启用`PID namespace`共享，它为每一个`Pod`提供1号进程，并收集`Pod`内的僵尸进程
2. 从`PID`看`pause`容器
   - 在`UNIX`系统中，`PID`为1的是`init`进程，即所有进程的父进程，`init`进程维护一张进程表并且不断地检查其它进程的状态。`init`进程的其中一个作用是当某个子进程由于父进程的错误退出而变成了孤儿进程，便会被`init`进程收养并在该进程退出时回收资源。
   - 在容器中，必须有一个进程充当每个`PID namespace`的`init`进程，使用`Docker`的话，`entrypoint`进程是`init`进程
3. 在`kubernetes`中使用`PID namespace`共享/隔离
   - 在`Kubernetes1.8+`，默认情况下禁用共享`PID namespace`，使用`--docer-disable-shared-pid=false`启用

四、`Kubernetes`网络驱动

1. `Kubernetes`支持的两种网络驱动

   - `CNI plugins`：遵守`appc/CNI`规范，允许自由接入多个符合`CNI`标准的网络插件
   - `kubenet plugins`：基于`cbr0`的一个单机容器网络方案，同时使用`CNI`的`bridge`和`host-local`实现一些功能

2. `kubenet`特色功能

   - 定制`MTU`

     `Kubenet`支持用户使用`Kubenet`的`--network-plugin-mtu`参数指定`MTU`

   - 带宽控制：

     支持通过`kubernetes.io/ingress-bandwidth`和`kubernetes.io/egress-bandwidth`这两个`annotations`设置`Pod`网络带宽限制，例如

     ```yaml
     apiVersion: v1
     kind: Pod
     metadata: 
       name: test
       annotations:
         kubernetes.io/ingress-bandwidth: 1M
         kubernetes.io/egress-bandwidth: 1M
     spec:
       containers:
       - name: test
         image: nginx:1.14-alpine
     ```

3. `CNI`基础

   1. 概述

      - `CNI`是容器网络的标准化，试图通过`JSON`描述一个容器网络配置
      - `CNI`是`Kubernetes`与底层网络插件之间的一个抽象层，为`Kubernetes`屏蔽了底层网络实现的复杂度，同时解耦了`kubernetes`的具体网络插件实现

   2. `CNI`接口

      - 创建容器时调用的配置网络接口

        ```go
        AddNetwork(net *NetworkConfig,rt* RuntimeConf)(types.Result,error)
        ```

      - 删除容器时调用的清理网络接口

        ```go
        DelNetwork(net *NetworkConfig,rt* RuntimeConf)
        
        // runtime配置主要是容器运行时传入的网络namespace信息
        ```

   3. 安装和使用`CNI`

      - 安装`CNI`

        `yum -y install kubernetes-cni`

      - 自定义网络

        ```shell
        cat > /etc/cni/net.d/10-mynet.conf <<EOF
        {
                "name": "mynet",
                "type": "bridge",
                "bridge": "cni0",
                "isGateway": true,
                "ipMasq": true,
                "ipam": {
                        "type": "local-host",
                        "subnet": "10.10.0.0/16",
                        "route": [
                                { "dst": "0.0.0.0/0" }
                        ]
                }
        }
        EOF
        ```

        - 在一个`Json`文件中，定义了名为`mynet`，它是一个`bridge`模型，`IP`地址管理`ipam`使用的是`host-local`(在本地用一个文件记录已经分配的容器`IP`地址)
        - `Kubenets`中的使用方法
          - `/etc/cni/net.d`用来存储`CNI`配置文件
          - `/opt/cni/bin`目录用来存放`CNI`插件的为进制文件

      - 在`Kubernetes`中使用`CNI`

        - `Kubelet`要使用`CNI`网络驱动需要配置启动参数`--network-plugin=cni`
        - 如果目录中有多个文件，则使用文件名字典序列中的第一个文件

      - `CNI`从`kubernetes1.11`开始支持`Pod`带宽控制，实现方式如下

        ```yaml
        apiVersion: v1
        kind: Pod
        metadata: 
          name: test
          annotations:
            kubernetes.io/ingress-bandwidth: 1M
            kubernetes.io/egress-bandwidth: 1M
        spec:
          containers:
          - name: test
            image: nginx:1.14-alpine
        ```

        - `Kubernetes`会自动为这个`Pod`分别限制上传和下载的带宽为`1Mb/s`，注意需要写`CNI`配置文件去调用`CNI`的默认`bandwidth`插件

          ```shell
          vim /etc/cni/net.d/my-net.conf
          {
          	"type": "bandwidth",
          	"capabilities": {"bandwidth": true}
          }
          ```

        - 用户通过`Pod`的`Annotations`下发带宽限制数值，`CNI`的`bandwidth`插件调用`Linux`流量控制插件`tc`，在宿主机上应用`tc`配置。

五、从集群内访问服务

1. `Service`总结

   - `kubernetes`使用`Labels`将多个相关的`Pod`组合成一个逻辑单元，称为`Service`.
   - `kubernetes`的`service`代表的是`kubernetes`后端服务的入口，它主要包含服务的访问`IP`和端口，工作在`L4`.
   - `Service`通过`Label Selector`选择与之匹配的`Pod`，被`Service`选中的`Pod`，当它们运行且能对外提供服务后，`kubernetes`的`Endpoint Controller`会生成一个新的`Endpoint`对象，记录`Pod`的`IP`和端口。
   - 当`Service`的后端`Pod`就绪后，`Kubernetes`会生成一个新的`Endpoints`对象，而且这个对象和`Service`同名。
   - `Service`的访问`IP`和`Endpoins/Pod IP`都会在`Kubernetes`的`DNS`服务器里存储域名和`IP`的对应关系。
   - `Kubernetes`使用`Kube-proxy`组件管理各服务于之后端`Pod`的链接，该组件在每个节点上运行。
   - `Kube-proxy`是一个基于出站流量的负载平衡控制器，它监控`Kubernetes API Service`并持续将服务`IP`(包括`Cluster IP`)映射到运行状况良好的`Pod`，落实到主机上就是`iptables/IPVS`等路由规则。
   - 如果希望回话保持，可以把`service.spec.sessionAffinity`设置为`ClientIP`，即基于客户端的会话保持

2. `service`的三个`port`

   - `port`：`Service`暴露的服务端口，也是客户端访问用的端口。
   - `targetPort`：应用程序实际监听`Pod`内流量的端口。
   - `nodePort`：`Kubernetes`提供给集群外部访问`Service`入口的一种方式。

3. `Service`的三种类型

   - `ClusterIP`：默认`Service`类型，主要作用是方便集群内`Pod`到`Pod`之间的调用。
   - `LoadBalancer`
   - `NodePort`：`NodePort`的实现机制是`kube-proxy`会创建一个`iptables`规则，所有访问本地`NodePort`的网络包都会被直接转发至后端`Service`的`Port`，`NodePort`会在主机上打开(但不监听)一个实际的端口。

4. `Kubernetes` 的服务发现：通过`DNS`实现

5. 无头服务(`headless`)：

   - 无头(`headless`)`Service`即没有`selector`的`Service`，`Service`抽象了该如何访问`Kubernetes Pod`，也能够抽象其它类型的`backend`，例如

     - 希望在生产环境中使用外部的数据库系统，但在测试环境用自己的数据库
     - 希望服务指向另一个`namespace`中或其它集群中的服务。
     - 正在将工作负载转移到`Kubernetes`集群，以及运行在`kubernetes`集群之外的`backend`

   - 定义示例

     ```yaml
     apiVersion: v1
     kind: Service
     metadata:
       name: my-service
     spec:
       ports:
       - protocol: TCP
         port: 80
         targetPort: 9376
     ```

     这个`Service`没有`Selector`，就不会创建相关的`Endpoints`对象，可以手动将`Service`映射到指定的`Endpoints`

     ```yaml
     apiVersion: v1
     kind: Endpoints
     metadata:
       name: my-service  #和service同名
     subsets:
       - addresses:
         - ip: 1.2.3.4
         ports:
         - port: 9376
     ```

6. `ExternalName Service`

   - 对于运行在集群外部的服务，它通过返回该外部服务的别名的方式提供服务

   - 示例

     ```yaml
     apiVersion: v1
     kind: Service
     metadata: 
       name: my-service
       namespace: prod
     spec:
       type: ExternalName
       externalName: my.database.example.com
     ```

     当查询主机`my-service.prod.svc.cluster.local`时，集群的`DNS`将返回一个值为`my.database.example.com`的`CNAME`记录。

7. 怎么访问本地服务

   - 当访问`NodePort`或`Load Balancer`类型`Service`的流量到底节点时，流量可能会被转发到其它节点上的`Pod`。这可能需要额外一跳的网络，如果要避免额外的跃点，则用户可以指定流量必须转到最初接收流量的节点上的`Pod`。
   - 将`service.spec.externalTrafficPolicy`设置为`Local`时，负载平衡器仅将流量发送到具有属于服务的正常`Pod`所在的节点。

8. 从集群外访问服务

   - `Kubernetes Ingress`：`Kubernetes`的`Ingress`资源对象是指授权入站连接到达集群内服务的规则集合

