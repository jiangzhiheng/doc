一、`Docker`的网络模式

1. 容器的网络方案种类
   - 单机的容器间通信
   - 跨主机的容器间通信
   - 容器与主机间通信
2. `Docker`四种网络模式
   - `brifge`模式：通过`--network=bridge`指定
   - `host`模式：通过`--network=host`指定
   - `container`模式：通过`--network=container:NAME_or_ID`指定，即`joiner`容器
   - `none`模式：通过`--network=none`指定
3. `Bridge`模式
   - `bridge`是`Docker`默认网桥，创建的容器的`veth pair`一端接在`docker0`网桥上
   - 连接在`docker0`上的所有容器默认网关均为`docker0`
4. `host`模式
   - 连接到`host`网络的容器共享`Docker host`的网络栈，容器的网络配置与`host`完全一致
   - 缺点：
     - 容器没有独立，隔离的网络栈，容器因与宿主机共享网络栈而争抢网络资源，并且容器奔溃也可能使主机崩溃，导致网络的隔离性不好
     - 端口资源冲突，宿主机上已经使用的端口就不能在用了
5. `container`模式
   - 在创建新的容器时制定容器的网络和一个已经存在的容器共享一个`network namespace`
   - 两个容器的进程通过`lo`网卡设备通信
6. `none`模式
   - `none`模式下的容器只有`lo`回环接口，没有其它网卡。

二、`Docker`网络常用操作

1. 查看容器`IP`

   `docker inspect -f "{{ .NetworkSettings.IPAddress }}"`

2. 端口映射

   - 在使用`docker run`的时候通过指定`-p`或`-P`参数进行容器和主机之间的端口映射

     - `-p`：需要指定主机端口和要映射到容器的哪个端口`HOST_PORT:CONTAINER_PORT`
     - `-P`：`Docker`随机将一个49000-49900之间的端口映射到内部容器开放的网络端口

   - 原理：在本地的`iptables`的`nat`表中添加相应的规则，将访问本机`IP:PORT`的网包进行一次`DNAT`，转换成`containerIP:containerPort`

   - 示例

     ```shell
     docker run -d -p 8080:80 nginx:1.14-alpine
     iptables -t nat vnL
     Chain DOCKER (2 references)
      pkts bytes target     prot opt in     out     source               destination         
         0     0 RETURN     all  --  docker0 *       0.0.0.0/0            0.0.0.0/0           
         0     0 DNAT       tcp  --  !docker0 *       0.0.0.0/0            0.0.0.0/0            tcp dpt:8080 to:172.17.0.3:80
     ```

   - 可以使用如下命令查看映射关系

     `docker port <container> <port number>`

3. 访问外网

   - 确认宿主机`ip_forward`是否打开
   - `Docker`在`iptables`的`POSTROUTING`链上创建规则，从容器网段出来访问外网的包，都要做一次`MASQUERADE`，即出去的包都用主机的`IP`地址替换源地址

4. `DNS`和主机名

   - 容器中的`DNS`和主机名一般通过三个系统配置文件维护

     - `/etc/resolv.conf`：在创建容器的时候，默认与主机的`/etc/resolv.conf`保持一致
     - `/etc/hosts`：记录容器自身的一些地址和名称
     - `/etc/hostname`：记录容器的主机名

   - 通过修改`/etc/docker/daemon.json`指定除主机`/etc/resolv.conf`以外的`DNS`

     ```json
     {
         "dns" : [
             "114.114.114.114",
             "8.8.8.8"
         ]
     }
     ```

   - 在`docker run`时通过`--hostname HOSTNAME`为容器指定`hostname`

5. 自定义网络

   - 创建自定义网络

     `docker network create -d bridge --subnet 192.168.0.0/16 mynet`

   - 查看网络详细信息

     `docker network inspect mynet`

   - 删除某个网络

     `docker network rm mynet`

   - 连接一个容器到网络中

     `docker network connect mynet CONTAINER_NAME`

   - 将容器和网络断开

     `docker network disconnect`

6. 发布服务：类似于`k8s`中的`service`

   - 创建一个网络

     `docker network create -d bridge foo`

   - 在这个网络里发布一个服务

     `docker service publish my-service.foo`

   - 将这个服务和容器绑定

     `docker service attach <container-id> my-service.foo`

7. `docker link`：两两互联

   - `docker link`就是把两个容器连起来相互通信，用法如下

     `docker run -d nginx --link=<container name or ID>:<alias>`

   - 示例

     ```shell
     docker run -d --name con1 nginx:1.14-alpine
     docker run -d --name con2 --link con1:source nginx:1.14-alpine
     docker exec -it con2 /bin/sh
     / # ping -c 3 con1
     PING con1 (172.17.0.4): 56 data bytes
     64 bytes from 172.17.0.4: seq=0 ttl=64 time=0.118 ms
     64 bytes from 172.17.0.4: seq=1 ttl=64 time=0.203 ms
     
     / # ping -c 3 source
     PING source (172.17.0.4): 56 data bytes
     64 bytes from 172.17.0.4: seq=0 ttl=64 time=0.071 ms
     64 bytes from 172.17.0.4: seq=1 ttl=64 time=0.251 ms
     
     / # cat /etc/hosts
     127.0.0.1	localhost
     172.17.0.4	source acb8d60ab2e3 con1
     172.17.0.5	1eda80d49ba7
     ```

三、容器第一个网络标准`CNN`(`Container Net-work Model`)

1. `CNM`中的主要概念：

   - `Network Sandbox`：容器网络栈，包括网卡，路由表，`DNS`配置等，对应的实现有`network namespace,FreeBSD Jail`等
   - `Endpoint`：`Endpoint`作为`Sandbox`接入`Network`的介质，是`Network Sandbox`和`Backend Network`之间的桥梁，对应的技术实现由`veth pair设备，tap/tun设备OVS内部端口等`
   - `Backend Network`：一组可以直接相互通信的`Endpoint`集合。对应的技术实现有`Linux Bridge,VLAN`等
   - `Network Controller`：对外提供分配及管理网络的`APIs`，`Docker Libnetwork`支持多个网络驱动，`Network Controller`运行绑定特定的驱动到指定的网络
   - `Driver`：网络驱动对用户而言是不直接交互的，它通过插件式的接入方式，提供最终网络功能的实现，`Driver`负责一个`Network`的管理，包括资源分配和回收。

2. `Libnetwork`

   - `Libnetwork`是`Docker`团队将`Docker`的网络功能，从`Docker`核心代码中分离出去，用`Go`语言实现的一个独立库
   - `Libnework`通过插件的形式为`Docker`容器提供网络功能，用户可以根据自己的需求实现自己的网络驱动，以便提供不同的网络功能
   - 从架构上看，`Libnetwork`为`Docker Daemon`和网络驱动提供了接口。
   - `Libnetwork`的网络控制器(`Network Controller`)负责将网络驱动和一个`Docker`网络进行对接
   - 网络驱动按提供方被划分为原生驱动和远程驱动(第三方插件)，原生驱动包括`none,bridge,overlay,Macvlan`，也可以按适用范围划为本地的和跨主机的。
   - `Libnetwork`要达到的效果应该是：
     - 用户可以创建一个或多个网络，一个容器可以加入一个或多个网络
     - 同一个网络中的容器可以通信，不同网络中的容器隔离。

3. `Libnetwork`扩展

   `remote`网络驱动需要实现以下接口，就能支持`Docker`容器网络的生命周期管理

   ```go
   Plugin.Activate
   Plugin.Deacticate
   NetworkDriver.GetCapalibities
   NetworkDriver.CreateNetwork
   NetworkDriver.DeleteNetwork  //删除Network
   NetworkDriver.CreateEndpoint  //为容器创建网路接口
   NetworkDriver.DeleteEndpoint
   NetworkDriver.EndpointOperInfo
   NetworkDriver.Join
   NetworkDriver.Leave  //容器与外部网络驱动解绑
   ```

   以思科的`Contiv`项目为例，`Contiv`是思科主导开发的以远程插件的形式，基于`OVS`提供`Docker`容器网络的`SDN`能力，功能上支持`VLAN,VXLAN,QoS`。要想使用`Contiv`,首先需要在`/var/docker/plugins/`目录下注册`socket`文件，每次处理`Libnetwork`的`RPC`请求时，通过`ovsdb-server`的管理接口执行修改`ovs`流表的操作

四、容器网络的虚拟化方案

1. 隧道方案
   - 特点：随着节点规模的增长，复杂度也会随之增加，而且用到了封包，因此出现网络问题定位起来比较复杂
   - 典型的`overlay`插件有：
     - `Weave`
     - `Open vSwitch()OVS`：基于`VXLAN`和`GRE`协议，但是性能方面损失比较严重
     - `flannel`：源于`CoreOS`，支持自研的`UDP`封包及`Linux`内核的`VXLAN`协议
2. 路由方案
   - 典型插件：
     - `Calico`
     - `Macvlan`
     - `Metaswitch`

   