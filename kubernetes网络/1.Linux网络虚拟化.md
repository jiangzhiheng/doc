一、网络虚拟化基础：`namespace`

1. `Linux`的`namespace`：

   - 作用：隔离内核资源
   - 种类：
     - `Mount namespace`：文件系统挂载点
     - `UTS namespace`：主机名
     - `IPC namespace`：`POSIX`进程间通信消息队列
     - `PID namespace`：进程`PID`数字空间
     - `network namespace`：`IP`地址
     - `user namespace`：`user ID`
   - 特点：`Linux`的`namespace`给里面的进程造成了两个错觉：
     - 它是系统里唯一的进程
     - 它独享系统的所有资源
   - 默认情况下，`Linux`进程处在和宿主机相同的`namespace`,即初始的根`namespace`里，默认享有全局系统资源

2. `network namespace`

   - 作用：隔离`Linux`系统的设备，以及`IP`地址、端口、路由表、防火墙规则等资源，因此每个网络`namespace`中都有自己的网络设备（如`IP`地址、路由表，端口范围、`/proc/net`目录等）

3. `network namespace`的创建

   - 通过`Linux`的`ip`工具的`netns`子命令

     ```shell
     [root@docker01 ~]# ip netns help
     Usage: ip netns list
            ip netns add NAME
            ip netns set NAME NETNSID
            ip [-all] netns delete [NAME]
            ip netns identify [PID]
            ip netns pids NAME
            ip [-all] netns exec [NAME] cmd ...
            ip netns monitor
            ip netns list-id
     ```

     ```shell
     # 创建一个network namespace
     ip netns add netns1
     # 查看系统有哪些network namespace
     ip netns list
     # 进入namespace执行操作
     ip netns exec netns1 ip link list
     # 删除创建network namespace
     ip netns delete netns1
     
     # 当ip命令创建了一个network namespace时，系统会在/var/run/netns路径下自动生成一个挂载点，挂载点的作用一方面是对namespace的管理，另一方面是使namespace没有进程运行也能存在
     ```

4. 配置`network namespace`

   1. 启用`lo`接口，默认`down`

      ```shell
      # ip netns exec netns1 ip link set dev lo up
      # ip netns exec netns1 ping 127.0.0.1 
      PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.
      64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.039 ms
      ```

   2. 创建`veth pair`

      ```shell
      # 创建veth pair
      ip link add veth0 type veth peer name veth1
      # 将veth1放到netns1 namespace中
      ip link set veth1 netns netns1
      
      [root@docker01 ~]# ip netns exec netns1 ip link list
      1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
          link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
      5: veth1@if6: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
          link/ether fe:ce:e7:4b:9f:a3 brd ff:ff:ff:ff:ff:ff link-netnsid 0
      ```

   3. 配置`veth pair`

      ```shell
      # 给netns1中的veth1设置ip地址并up
      ip netns exec netns1 ifconfig veth1 10.1.1.1/24 up
      # 给veth pair的另一半(位于宿主机)设置ip地址
      ifconfig veth0 10.1.1.2/24 up
      ```

   4. 双向测试

      ```shell
      [root@docker01 ~]# ping 10.1.1.1
      64 bytes from 10.1.1.1: icmp_seq=2 ttl=64 time=0.094 ms
      [root@docker01 ~]# ip netns exec netns1 ping 10.1.1.2
      64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=0.201 ms
      ```

   5. 另外，不同`network namespace`之间的路由表和防火墙等也是隔离的

   6. `Tips`

      - 进程可以通过`Linux`系统调用`clone(),unshare()和setns`进入`network namespace`

      - 非`root`进程被分配到`network namespace`后只能访问和配置已经存在于该`network`的设备

      - `root`进程可以在`network namespace`里创建新的网络设备

      - `network namespace`里的`root`进程还能把本`network namespace`的虚拟网络设备分配到其它`network namespace`,这个路径可以从主机的根`network namespace`到用户自定义的`namespace`,反之亦可

      - 请看以下命令

        ```shell
        ip netns exec netns1 ip link set veth1 1
        
        # 以上命令的作用是进入netns1，将veth1 移动到PID为1(init进程)所在的network namespace
        # 所以这会造成一定的安全风险，对namespace的root用户而言，它们都可以把namespace里的虚拟网络设备移动到其它network namespace中，甚至包括主机根network namespace！！，如果需要屏蔽这一行为，则需要结合PID namespace和Mount namespace
        ```

5. `network namespace API`的使用

   `clone(),unshare(),和setns()`系统调用会使用`CLONE_NEW*`来区别要操作的`namespace`类型，`CLONE_NEW*`常量一共有6个，分别是`CLONE_NEWIPC,CLONE_NEWNS,CLONE_NEWNET,CLONE_NEWPID,CLONE_NEWUSER,CLONE_NEWUTS`

   1. 创建`namespace`：`clone`系统调用

   2. `/proc/PID/ns`目录：维持`namespace`存在

      - 每个`Linux`进程都拥有一个属于自己的`/proc/PID/ns`,这个目录下的每一个文件都代表了一个类型的`namespace`,这些文件提供了操作进程关联`namespace`的一种方式

        ```shell
        [root@docker01 ~]# ll /proc/1/ns/
        total 0
        lrwxrwxrwx 1 root root 0 Mar  3 01:46 ipc -> ipc:[4026531839]
        lrwxrwxrwx 1 root root 0 Mar  3 01:46 mnt -> mnt:[4026531840]
        lrwxrwxrwx 1 root root 0 Mar  3 01:46 net -> net:[4026531956] # 代表network namespace
        lrwxrwxrwx 1 root root 0 Mar  3 01:46 pid -> pid:[4026531836]
        lrwxrwxrwx 1 root root 0 Mar  3 01:46 user -> user:[4026531837]
        lrwxrwxrwx 1 root root 0 Mar  3 01:46 uts -> uts:[4026531838]
        
        # 符号链接的作用：
        # 1. 确定某两个进程是否属于同一个namespace，如果两个进程属于同一个namespace，则符号链接的inode数字会是一样的
        # 2. 只要打开文件描述符，不需要进程存在也能保持namespace存在，例如
        touch /my/net	#新建一个文件
        mount --bind /proc/$$/ns/net /my/net
        # 如上所示，把/proc/PID/ns目录下挂载起来就能起到打开文件描述符的作用，而且这个network namespace会一直存在，直到/proc/self/ns/net被卸载
        ```

   3. 往`namespace`中添加进程，`setns`系统调用

      - `setns()`的定义如下

        ```c
        int setns(int fd,int nstype);
        /*
        * fd：进程待加入的namespace对应的文件描述符
        * nstype的作用检查fd的类型是否符合要求
        */
        ```

   4. `unshare`系统调用：帮助进程逃离`namespace`

      - `unshare()`的定义

        ```c
        int unshare(int flags);
        ```

      - `unshare`工作机制

        先通过指定`flags`参数创建相应的`namespace`,再把这个进程挪到新创建的`namespace`中

      - `Linux`中的`unshare`命令

        ```shell
        [root@docker01 ~]# unshare --help
        Usage:
         unshare [options] <program> [<argument>...]
         
        Run a program with some namespaces unshared from the parent.
        
        Options:
         -m, --mount               unshare mounts namespace
         -u, --uts                 unshare UTS namespace (hostname etc)
         -i, --ipc                 unshare System V IPC namespace
         -n, --net                 unshare network namespace
         -p, --pid                 unshare pid namespace
         -U, --user                unshare user namespace
         -f, --fork                fork before launching <program>
             --mount-proc[=<dir>]  mount proc filesystem first (implies --mount)
         -r, --map-root-user       map current user to root (implies --user)
             --propagation <slave|shared|private|unchanged>
                                   modify mount propagation in mount namespace
         -s, --setgroups allow|deny  control the setgroups syscall in user namespaces
        
         -h, --help     display this help and exit
        ```

   

二、`veth pair`详解

1. `veth pair`概念及基本操作

   - 概念：`veth`是虚拟以太网卡`(Virtual Ethernet)`的缩写，`veth`设备总是成对的，因此我们称之为`veth pair`。`veth pair`一端发送的数据会在另一端接收，非常像`Linux`的双向管道

   - 常用操作:

     ```shell
     # 创建veth pair
     ip link add veth0 type veth peer name veth1
     # 查看veth设备
     ip link list
     # 将设备状态设置为up
     ip link set dev veth0 up
     ip link set dev veth1 up
     # 为设备配置ip地址
     ifconfig veth0 192.168.1.1/24
     # 将veth设备放置到名称空间中
     ip link set veth1 netns netns1
     ```

2. 容器与`host veth pair`的关系

   - 查看容器中网卡与主机`veth`成对关系的方法
     - 对比容器内`eth0`网卡` cat /sys/class/net/eth0/iflink`的值，遍历主机上`/sys/class/net`下面的全部目录，招待与容器中`iflink`值相同的`veth`的名字

三、`Linux Bridge`

1. 概念

   - 网桥是二层网络设备，两个端口分别有一条独立的交换信道，不共享一条背板总线，可隔离冲突域
   - 连个`network namespace`之间可以通过`veth pair`实现，但多个`network namespace`相连就需要`bridge`
   - `Linux bridge`有多个端口，数据可以从任何端口进来，进来之后从哪个端口出去取决于目的`MAC`,原理和物理交换机差不多

2. `Linux bridge`基本操作

   ```shell
   # 创建一个网桥br0
   ip link add name br0 type bridge
   ip link set br0 up
   # 创建一对veth pair设备，并配置ip地址
   # 将veth0放到br0上
   ip link set dev veth0 master br0
   ```

3. `Linux bridge`在网络虚拟化中的应用

   - 虚拟机

     虚拟机通过`tun/tap`设备，将虚拟机内的网卡与`br0`连接起来，虚拟机发出去的数据包先到达`br0`，然后有`br0`交给`eth0`发送出去，数据包都不需要经过`host`机器的协议栈，效率高

   - 容器

     容器中使用的`veth pair`设备，而虚拟机使用的是`tun/tap`设备，在虚拟机场景下，虚拟机一般会和主机在同一网段，而在容器场景下，容器和物理网络不再同一个网段内

4. 网络接口的混杂模式

   - 概念：

     - 混杂模式，`(Promiscuous mode)`，简称`Promisc mode`，俗称"监听模式"，通常被网络管理员用来诊断网络问题

     - 混杂模式是指一个网卡会把它接收的所有流量都交给`cpu`，而不是只把它像转交的部分交给`cpu`.

       ```shell
       # 启用网卡的混杂模式
       [root@docker01 ~]# ifconfig ens33 promisc
       [root@docker01 ~]# ifconfig ens33
       ens33: flags=4419<UP,BROADCAST,RUNNING,PROMISC,MULTICAST>  mtu 1500
       # 使网卡退出混杂模式
       [root@docker01 ~]# ifconfig ens33 -promisc
       # 将网络设备加入Linux bridge后，会自动进入混杂模式
       [root@docker01 ~]# dmesg |grep promiscuous
       [ 2534.759461] device veth0 entered promiscuous mode
       [ 4241.507234] device ens33 entered promiscuous mode
       ```

四、 `tun/tap`设备

   - 概念：
     - 从`Linux`文件系统角度来看，它是用户可以用文件句柄操作的字符设备
     - 从网络虚拟化角度看，它是虚拟网卡，一端连接着网络协议栈，一端连接着用户态程序
     - 从网络协议栈的角度看，`tun/tap`设备这类虚拟网卡与物理网卡并无区别，只是对`tun/tap`设备而言，它与物理网卡的不同表现在它的数据源不是物理链路，而是来自***用户态***
     - 普通的网卡是通过网线来收发数据包的话，而 ***`tun/tap`*** 设备比较特殊，它通过一个文件收发数据包

   1. `tun(tunnel隧道)`设备：
      - 工作模式：
        - `tunX`和`eth0`在逻辑上是等价的，`tunX`这个接口是系统通过软件模拟出来的
        - 网卡接口`tunX`所代表的虚拟网卡通过文件`/dev/tunX`与我们的应用程序相连，应用程序每次通过`write`之类的系统调用将数据写入该文件，这些数据会以网络层数据包的形式，通过该虚拟网卡，经由网络接口`tunX`传递给网络协议栈，同时程序也可通过`read`之类的系统调用，经由文件`/dev/tunX`读取到协议栈向`tunX`穿都的所有数据包
        - 协议栈可以像操纵普通网卡一样来操纵 `tunX` 所代表的虚拟网卡。比如说，给 `tunX` 设定 `IP` 地址，设置路由
   2. `tap`设备
      - `tun`设备是一个三层设备，它只模拟到了`IP`层，即网络层我们可以通过`/dev/tunX`文件收发`IP`层数据包，它无法与物理网卡做`bridge`，但是可以通过三层交换(如`ip_forward`)与物理网卡连通，可以使用`ifconfig`之类的命令给设备设定`IP`地址
      - `tap`设备是一个二层设备，它比`tun`更加深入，通过`/dev/tapX`文件可以手法`MAC`层数据包，即数据链路层，拥有`MAC`层功能，可以与物理网卡做`bridge`，支持`MAC`层广播，同样的，我们可以通过`ifconfig`之类的命令给设备设定`IP`地址，我们也可以给它设定`MAC`地址

五、 `iptables`

   1. `netfilter`：作为一个通用的，抽象的框架，提供一整套`hook`函数的管理机制

      - `IP`层的5个钩子点的位置，对应于`iptables`五条链，分别是`PREROUTING,POSTROUTING,INPUT,OUTPUT,FORWARD`
      - `netfilter`是`Linux`内核网络模块的一个经典框架

   2. `iptables`之`table,chain,rule`

      - `iptables`是用户空间的一个程序，通过`netlink`和内核的`netfilter`框架打交道，负责往钩子上配置回调函数

      - `iptables`中的5条链

        - `INPUT`：一般用于处理输入本地进程的数据包
        - `OUTPUT`：一般用于处理本地进程的输出数据包
        - `FORWARD`：一般用于处理转发到其他机器或其它`network namespace`的数据包
        - `PREROUTING`：可以在此处进行`DNAT`
        - `POSTROUTING`：可以在此处进行`SNAT`

      - `iptables`中的5张表

        - `filter`：用于控制到达某条链上的数据包是否继续放行，直接丢弃(`drop`)，或拒绝(`reject`)
        - `nat`：用于修改数据包的源和目的地址
        - `mangle`：用于修改数据包的`IP`头信息
        - `raw`：`iptables`是有状态的，即`iptables`对数据包有链接追踪机制，而`raw`是去除这种机制的
        - `security`：最不常用的表，用于在数据包上应用`SELinux`

        这五张表的优先级从高到低是：`raw,mangle,nat,filter,security`

      - `iptables`的规则：

        - 匹配条件：协议类型，源`IP`，目标`IP`，源端口，目标端口，连接状态等
        - 动作：`DROP(丢弃),REJECT(拒绝),QUEUE(放入用户空间队列),RETURN(),ACCEPT,JUMP(跳转到其它用户自定义链继续执行)`

   3. `iptables`常规用法：

      1. 查看所有`iptables`规则

         ```shell
         # 列出iptables的所有规则
         iptables -L -n   
         # 默认输出的filter表中的规则，如果想要看nat表中的规则，则：
         iptables -t nat -L -n
         # 一般情况下还可使用-v列出详细信息
         iptables -nvL
         ```

         - `iptables`的每条链下面的规则处理顺序是从上到下逐条遍历的，除非遇到`DEOP,RETURN,REJECT`这些动作

      2. 配置内置链的默认策略

         ```shell
         # 默认的不让进
         iptables --policy INPUT DROP
         # 默认的不允许转发
         iptables --policy FORWARD DROP
         # 默认的可以出去
         iptables --policy OUTPUT ACCEPT
         ```

      3. 配置防火墙规则策略

         - 默认策略是全通`ACCEPT`，则需要定义一些策略来封堵(黑名单)

         - 默认策略是全不同`DROP`，则需要定义一些策略来解封(白名单)

         - 配置示例：

           ```shell
           ## 1.配置允许SSH连接
           iptables -A INPUT -s 192.169.1.0/24 -p tcp --dport=22 -j ACCEPT
           # -A :以追加的方式增加这条规则
           # 允许来自192.169.1.0/24网段的包发送到本地tcp 22号端口
           # 也可通过iptables -I [chain] [number]将规则Insert到链的指定位置
           
           ## 2.阻止来自某个网段的所有的包
           iptables -A INPUT -s 10.0.0.0/24 -j DROP #阻止10.0.0.0/24网段的所有包
           iptables -A INPUT -s 10.0.0.6 -j DROP    #组织单个ip
           
           ## 3.封锁端口
           iptables -A INPUT -p tcp --dport 1234 -j DROP #阻止外部访问本地1234端口
           iptables -A OUTPUT -p tcp --dport 1234 -j DROP #阻止本地1234端口对外访问
           
           ## 4.端口转发
           iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 8080
           # 将从eth0口进来的目标端口是80的数据包重定向到8080端口
           
           ## 5.禁用ping
           iptables -A INPUT -p icmp -j DROP
           
           ## 6.删除规则
           iptables -F #清除当前所有规则
           iptables -t nat -F #清楚nat表的规则
           iptables -D INPUT -s 10.0.0.10 -j DROP #清除单条规则
           
           ## 7.自定义链
           iptables -N BAR
           iptables -X FOO # 删除空链
           ```

      4. `DNAT`
      
         ```shell
         iptables -t nat -A PREROUTING -d 1.2.3.4 -p tcp --dport 80 -j DNAT --to-destination 10.20.30.40:8080
         # 注意：当涉及转发的目的IP是外机时，需要确保启用ip forward,把Linux当交换机用
         echo 1 >/proc/sys/net/ipv4/ip_forward
         ```
      
      5. `SNAT`/网络地址欺骗
      
         ```shell
         # 修改数据包的源IP地址
         iptables -t nat -A POSTROUTING -s 192.168.1.2 -j SNAT --to-source 10.0.0.12
         
         # 网络地址伪装，其实就是一种特殊的源地址转换，报文从哪个网卡出就用该网卡的IP地址替换该报文的源地址
         iptables -t nat -A POSTROUTING -s 10.9.0.0/16 -j MASQUERADE
         # 如果要控制被替换的源地址，则需要使用-o eth0指定报文从eth0口出并使用eth0网口的IP地址做源地址伪装
         ```
      
      6. 保存与恢复
      
         ```shell
         # iptables规则做出的改变是临时的，重启机器后就会丢失
         iptables-save #持久化到本地
         iptables-save > iptables.bak  #重定向到文件
         iptables-restore < iptables.bak  # 还原iptables规则
         ```

六、 `Linux`隧道：`ipip`

1. `Linux`支持的`L3`隧道：

   - `ipip`：即`IPv4 in IPv4`，在`IPv4`的报文上再封装一个`IPv4`报文
   - `GRE`：即通用路由封装(`Generic Routing Encapsulation`)，定义了在任意一种网络层协议上封装其它任意一种网络层协议的机制，适用于`IPv4`和`IPv6`
   - `sit`：类似于`ipip`，不同的是`sit`用`IPv4`封装`IPv6`，即`IPv6 over IPv4`
   - `ISATAP`：即站内自动隧道寻址协议(`Intra-Site Autimatic Tunnel Addressing Protocol`)，与`sit`类似，也用于`IPv6`的隧道封装
   - `VTI`：即虚拟隧道接口(`Virtual Tunnel Interface`)，是思科提出的一种`IPSec`隧道协议

2. 测试`ipip`隧道

   - 加载内核模块

     ```shell
     [root@docker01 ~]# modprobe ipip
     [root@docker01 ~]# lsmod |grep ipip
     ipip                   13465  0 
     tunnel4                13252  1 ipip
     ip_tunnel              25163  1 ipip
     ```

   - 实验过程：

     ```shell
     ## 创建两个network nemespace
     ip netns add ns1
     ip netns add ns2
     ## 创建两对veth pair，令其一段挂在namespace中
     ip link add v1 type veth peer name v1_p
     ip link add v2 type veth peer name v2_p
     ip link set dev v1 netns ns1
     ip link set dev v2 netns ns2
     ## 分别给veth -pair端点配上ip并up
     ip addr add 10.10.10.1/24 dev v1_p
     ip addr add 10.10.20.1/24 dev v2_p
     ip link set v1_p up
     ip link set v2_p up
     
     ip netns exec ns1 ip addr add 10.10.10.2/24 dev v1
     ip netns exec ns2 ip addr add 10.10.20.2/24 dev v2
     ip netns exec ns1 ip link set v1 up
     ip netns exec ns2 ip link set v2 up
     ## 打开核心转发
     echo 1 > /proc/sys/net/ipv4/ip_forward
     ## v1 ping v2 ?
     ip netns exec ns1 ping 10.10.20.2
     # 不通
     
     ## 查看ns1 路由表
     [root@docker01 ~]# ip netns exec ns1 route -n
     Kernel IP routing table
     Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
     10.10.10.0      0.0.0.0         255.255.255.0   U     0      0        0 v1
     
     ## 添加通往10.10.20.0/24网段的路由
     ip netns exec ns1 route add -net 10.10.20.0 netmask 255.255.255.0 gw 10.10.10.1
     # 添加回程路由
     ip netns exec ns2 route add -net 10.10.10.0 netmask 255.255.255.0 gw 10.10.20.1
     # 再次ping测试，隧道外层网络调通！！！
     [root@docker01 ~]# ip netns exec ns1 ping 10.10.20.2
     PING 10.10.20.2 (10.10.20.2) 56(84) bytes of data.
     64 bytes from 10.10.20.2: icmp_seq=1 ttl=63 time=0.081 ms
     64 bytes from 10.10.20.2: icmp_seq=2 ttl=63 time=0.128 ms
     
     ## 创建tun设备，并设置为ipip隧道
     # 在ns1上创建tun1 和ipip tunnel
     ip netns exec ns1 ip tunnel add tun1 mode ipip remote 10.10.20.2 local 10.10.10.2
     ip netns exec ns1 ip link set tun1 up
     ip netns exec ns1 ip addr add 172.16.100.10 peer 172.16.200.10 dev tun1
     # 1. 命令1：创建隧道设备tun1，并设置隧道端点，用remote和local表示
     # 2. 命令3：设置隧道内层IP，封装原始报文
     # 同理，在ns上创建tun2和ipip隧道
     ip netns exec ns2 ip tunnel add tun2 mode ipip remote 10.10.10.2 local 10.10.20.2
     ip netns exec ns2 ip link set tun2 up
     ip netns exec ns2 ip addr add 172.16.200.10 peer 172.16.100.10 dev tun2
     
     ## ping测试
     [root@docker01 ~]# ip netns exec ns1 ping 172.16.200.10
     PING 172.16.200.10 (172.16.200.10) 56(84) bytes of data.
     64 bytes from 172.16.200.10: icmp_seq=1 ttl=64 time=0.106 ms
     64 bytes from 172.16.200.10: icmp_seq=2 ttl=64 time=0.216 ms
     64 bytes from 172.16.200.10: icmp_seq=3 ttl=64 time=0.162 ms
     # 隧道通信成功！
     ```

3. 总结：

   1. `tun1`和`tun2`不在同一网段，如果要通信，要查看路由表，通过`ip tunnel`命令建立`ipip`隧道后，会自动生成一条路由

      ```shell
      [root@docker01 ~]# ip netns exec ns1 route -n
      Kernel IP routing table
      Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
      10.10.10.0      0.0.0.0         255.255.255.0   U     0      0        0 v1
      10.10.20.0      10.10.10.1      255.255.255.0   UG    0      0        0 v1
      172.16.200.10   0.0.0.0         255.255.255.255 UH    0      0        0 tun1
      
      # 去往172.16.200.10的报文直接从tun1出去了
      ```

   2. 由于配置了隧道端点，报文从`tun1`出来后到达`v1`，根据`ipip`隧道的配置，会封装上一层新的`IP`头部，源地址和目标地址为隧道端点的`IP`地址

七、`Linux`隧道网络：`VXLAN`

1. 概述：

   - `VXLAN(Virtual eXtensible LAN)`：虚拟可扩展的局域网，是一种虚拟化隧道通信技术，它是一种`overlay`技术，通过三层的网络搭建虚拟的二层网络。
   - `VXLAN`是一个一对多的网络，一个`VXLAN`设备能通过像网桥一样的学习方式学习到其它对端的`IP`地址，也可以配置静态转发表。

2. 出现背景：

   - 突破传统`VLAN` 4096个子网的限制，`VXLAN`报文`Header`有`24bit`，支持2的24次方个子网，通过`VNI(Virtual Network Identifier)`区分不同的子网
   - 多租网络隔离：不同用户之间需要独立的分配`IP`和`MAC`地址
   - 云计算环境下保证网络一直可用，也就是大二层的概念

3. `VXLAN`协议原理：

   - 创建在原来的`IP`(三层)网络上，只要三层可达（能够通过`IP`互相通信）的网络就能部署`VXLAN`
   - 在`VXLAN`网络的每个端点都有一个`VTEP`设备，负责`VXLAN`协议报文的封包和解包，也就是在虚拟报文上分装`VTEP`通信的报文头部
   - 物理网络上可以创建多个`VXLAN`网络，可以将这些`VXLAN`网络看作一个隧道，不同节点上的虚拟机/容器能够通过隧道直连
   - 通过`VNI`表示不同的 `VXLAN`网络，使得不同的`VXLAN`可以互相隔离。
   - `VXLAN`中的几个重要概念：
     - `VTEP(VXLAN Tunnel Endpoints)`：`VXLAN`网络的边缘设备，用来进行`VXLAN`报文的处理（封包和解包），`VTEP`可以是网络设备（例如交换机），也可以是一台主机
     - `VNI`：`(Virtual Network Identifier)`：`VNI`是每个`VXLAN`的标识，可以理解为`VLAN ID`
     - `Tunnel`：只是一个逻辑上的概念，并没有具体的物理实体相对应
   - `VXLAN`报文封装格式：原始的二层以太网帧，被放在`VXLAN`包头里进行封装，再套到标准的`UDP`头部(`UDP`头部，`IP`头部和`MAC`头部)
   - `UDP`目的端口是接收方`VTEP`设备使用的端口，`IANA`分配了4789作为`VXLAN`的目的`UDP`端口

4. `VXLAN`组网必要信息

   - 一个完整的`VXLAN`报文需要的信息
     - 内层报文：通信双方的`IP`地址已明确，`VXLAN`需要一个机制来实现`ARP`功能来填充对方的`MAC`地址
     - `VXLAN`头部：需要知道`VNI`
     - `UDP`头部：源地址和目的地址的端口，源地址由系统生成并管理，目的端口为4789
     - `IP`头部：对端`VTEP`的`IP`地址
     - `MAC`头部
   - 总结：`VXLAN`一共需要知道三个信息：内部`MAC`，`VTEP IP`和`VNI`
   - 获取`VXLAN`网络信息的两种方式：多播和控制中心

5. `VXLAN`基本配置命令

   ```shell
   ## 1. 创建VXLAN接口
   ip link add VXLAN0 type VXLAN id 42 group 239.1.1.1 dev eth0 dstport 4789
   # 使用eth0上的多播组239.1.1.1通信
   # 一般讲vxlan接口叫做VTEP
   # 多播组主要通过ARP泛洪来学习MAC地址
   # 如果网络不复杂，则可以认为某一节点上所有子网的IP的MAC、地址和节点上的VTEP的MAC地址一致，直接用VTEP MAC封装报文
   
   ## 2. 删除VXLAN接口
   ip link delete VXLAN0
   
   ## 3. 查看VXLAN接口信息
   ip -d link show VXLAN0
   
   ## 4. VXLAN的转发表项
   # 创建一条转发表项
   bridge fdb add to 00:17:42:8a:b4:05 dst 192.19.0.2 dev VXLAN0
   # 00:17:42:8a:b4:05是对端VTEP的MAC地址，192.19.0.2是对端VTEP的IP地址
   
   # 删除转发表项
   bridge fdb delete 00:17:42:8a:b4:05 dev VXLAN0
   # 查看VXLAN接口的转发表
   bridge fdb show dev VXLAN0
   ```

   补充：

   - 网络设备都以`MAC`地址唯一地表示自己，而交换机要实现设备之间的通信就必须知道自己的哪个端口连接着哪个设备，因此就需要一张`MAC`地址与端口号一一对应的表，即`FDB`表
   - `FDB`表主要有`MAC`地址，端口号，`VLAN`号一些标志域等信息组成
   - 如果收到数据帧的目的`MAC`地址不存在于`FDB`表中，则泛洪到所属`VLAN`的其它端口

6. `VXLAN`组网实验

   1. 点对点的 `VXLAN`

      ```shell
      ## 1.创建VXLAN接口
      ip link add vxlan0 type vxlan \
      	id 42 \
      	dstport 4789 \
      	remote 192.168.1.3 \  #对端VTEP的IP地址
      	local 192.168.1.2 \
      	dev eth0 作用和local参数相同，当前节点VTEP要使用的IP地址
      	
      ## 2.为刚创建的VXLAN网卡配置IP地址并up
      ip addr add 172.17.1.2/24 dev vxlan0
      ip link set vxlan0 up
      
      ## 3.查看路由和fdb表信息
      ip route
      172.17.1.0/24 dev vxlan0 proto kernel scope link src 172.17.1.2
      # 所有目的地址是172.17.1.0/24网段的都要通过vlan0转发
      bridge fdb
      00:00:00:00:00:00 dev vxlan0 dst 192.168.1.3 via ens33 self permanent
      # 默认的VTEP对端地址是192.168.1.3
      ## 4.在另外一台机器上进行相同的配置，要保证VNI和dstport相同
      ping -c 3 172.17.1.3
      ```

   2. 多播模式的`VXLAN`

      ```shell
      ## 1.创建vxlan接口
      ip link add vxlan0 type vxlan \
      	ip 42 \
      	dstport 4789 \
      	local 192.168.1.2 \
      	group 224.1.1.1 \
      	dev eth0
      ## 2.为刚创建的VXLAN网卡配置IP地址并up
      ip addr add 172.17.1.2/24 dev vxlan0
      ip link set vxlan0 up
      ```

      分析：

      - `group 224.1.1.1`表示将`VTEP`加入一个多播组多播地址是`224.1.1.1`
      - 一个`VXLAN`网络的`ping`报文要经历`ARP`寻址+`ICMP`响应两个过程，当然，`VTEP`设备学习到对方`ARP`地址后就可以免去`ARP`寻址过程

   3. `VXLAN`+桥接网络

      ```shell
      ## 1.创建vxlan接口
      ip link add vxlan0 type vxlan \
      	ip 42 \
      	dstport 4789 \
      	local 192.168.1.2 \
      	group 224.1.1.1 \
      	dev eth0
      ## 2.创建网桥，把VXLAN网卡vxlan0绑定到上面，并启动
      ip link add bridge0 type bridge
      ip link set vxlan0 master bridge
      ip link set vxlan0 up
      ip link set bridge0 up
      
      ## 3.创建network namespace 和一对veth pair，并将veth pair一端绑定到网桥，并把另一端放到namespace中，并绑定IP地址为172.17.1.2
      ## 4.在另一主机上做同样的配置测试
      ```

7. 分布式控制中心

   在分布式控制中心，一般情况下，这种架构在每个`VTEP`所在节点都运行一个`agent`，它会和控制中心通信，获取隧道通信所需要的信息并以某种方式告诉`VTEP`

8. 自维护`VTEP`组

   创建`VXLAN`网卡时，不指定`remote`或`group`参数，并通过手动维护`FDB`表项，手动维护一个`VTEP`多播组
   
   ```shell
   ip link add vxlan0 type vxlan \
   	id 42 \
   	dstport 4789 \
   	dev eth0
   # 然后手动添加FDB表象
   bridge fdb append 00:00:00:00:00:00 dev vxlan0 dst 192.168.8.101
   bridge fdb append 00:00:00:00:00:00 dev vxlan0 dst 192.168.8.102
   ```
   
   - 手动维护`FDB`表项
   - 手动维护`ARP`表项
   - 动态更新`FDB`和`ARP`表项

八、`Macvlan`

1. `Macvlan`五大工作模式解析

   `Macvlan`接口可以看作是物理以太网接口的虚拟子接口，`Macvlan`允许用户在主机的一个网络接口上配置多个虚拟的网络接口，每个`Macvlan`接口都有自己的区别于父接口的`MAC`地址，并且可以像普通网络接口一样分配`IP`地址。因此，使用`Macvlan`技术带来的效果是一块五力网卡上可以绑定多个`IP`地址，每个`IP`地址都有自己的`MAC`地址。

   - `bridge`模式

     - 类似于`Linux bridge`，是`Macvlan`最常用的模式，比较适合共享同一个父接口的`Macvlan`网卡直接通信，不需要把流量通过父接口发送到外部网络广播帧将会被泛洪到连接在“网桥”上的所有其它子接口和物理接口。
     - 缺点是如果父接口故障，所有`Macvlan`子接口会一起故障，子接口之间也将无法通信

   - `VEPA(Virtual Ethernet Port Aggregator)`，虚拟以太网端口聚合，是默认模式

     - 所有从`Macvlan`端口发出的流量，不管目的地址是什么，全部一股脑地发送给父接口----即使流量的目的地是共享同一个父接口的其它`Macvlan`接口。
     - 在二层网络下，由于生成树协议的原因，两个`Macvlan`之间的通信会被阻塞，这时就需要接入的外部交换机支持`hairpin`
     - 在`VEPA`模式下，从父接口接收到的广播包会泛洪给所有的子接口

     ```shell
     # 目前大多数交换机都不支持hairpin模式，但Linux可以通过一种hairpin模式的网桥，让VEPA模式下的Macvlan接口能够直接通信
     brctl hairpin br0 eth0 on  #配置Linux网桥br0，使得从eth0收到包后再从eth0发送出去
     
     # 使用ip命令直接设置网卡的hairpin模式
     ip set dev eth0 hairpin on
     
     # 通过写sysfs目录下设备文件设置br0网桥eth0端口的hairpin
     echo 1 > /sys/class/net/br0/brif/eth1/hairpin_mode
     
     # 配置了hairpin后，源地址和目的地址都是本地Macvlan接口地址的流量，会被Linux网桥发回给相应的接口
     ```

   - `Private`模式

     - `Private`模式类似于`VEPA`模式，但又增强了`VEPA`模式的隔离能力，其完全阻止共享同一父接口的`Macvlan`虚拟网卡之间的通信
     - 即使配置了`hairpin`，让从父接口发出的流量返回宿主机，相应的通信流量依然会被丢弃

   - `passthru`模式：直通模式

     - 每个父接口只能和一个`Macvlan`网卡绑定，并且`Macvlan`网卡继承父接口的`Mac`地址

   - `source`模式

     - 这种模式下，寄生在物理设备上，`Macvlan`设备只接收制定的源`MAC`地址的数据包，其它的一概丢弃

2. 测试使用`Macvlan`设备

   ```shell
   ## 创建bridge模式的macvlan设备
   ip link add eth0.1 link eth0 type macvlan mode bridge
   ## 查看macvlan网卡的详细信息
   ip -d link show eth0.1 
   ## 启用macvlan网卡
   ip link set eth0.1 up
   ## 删除macvlan设备
   ip link del eth0.1
   ```

3. `Macvlan`的跨机通信

   ```shell
   ## A节点192.168.1.2 ，B节点192.168.1.3
   
   ## 1.在A节点上创建一个不带网络初始化的容器
   docker run -d --net="none" --name=test1 busybox:latest
   # 获取新创建容器对应的PID
   docker inspect --format="" test1
   20845
   
   ## 创建macvlan设备
   ip link add eth0.1 link eth0 type macvlan mode bridge
   # 将创建的macvlan设备放入名称空间中
   ip link set netns 20845 eth0.1
   
   ## 进入namespace中配置eth0.1
   nsenter --target=20845 --net
   ip link set eth0.1 up
   ip addr add 192.168.1.12/24 dev eth0.1
   ip route add default via 192.168.1.254 dev eth0.1
   # 从主机B上可以ping通容器test1
   ping 192.168.1.12
   
   ##主机A无法ping通容器原因：在Macvlan世界中，物理网卡(父接口)相当于一个交换机，故物理网卡只转发数据包而不处理数据包，所以无法ping通
   ```

4. `Macvlan`和`overlay`对比

   - `Macvlan`和`overlay`作用范围不一样，`overlay`是全局范围类型的网络，`Macvlan`的作用范围只是本地
   - 每个宿主机创建的`Macvlan`网络是独立的，如果要实现跨主机通信，则必须满足
     - 通信两端的主机在网卡配置混杂模式
     - 两台主机上的`Macvlan`子网`IP`没有重叠
   
5. `Macvlan`的局限性

   - 无法支持大量的`MAC`地址
   - 无法工作在无线网中国

九、`IPvlan`

1. `IPvlan`特点

   - `IPvlan`所有的虚拟接口都有相同的`Mac`地址，而`IP`地址却各不相同
   - 所有的`IPvlan`虚拟接口共享`MAC`地址，所以特别需要注意`DHCP`的使用场景
   - `IPvlan`有`L2`和`L3`两种不同的模式，一个父接口只能选择其中一种模式，依附于它的所有子接口都运行在该模式下
   - `Linux`内核3.19版本才开始支持`IPvlan`，`Docker`从4.2版本开始支持`IPvlan`

2. `IPvlan`两种模式原理

   1. `L2`模式

      `IPvlan L2`模式和`Macvlan bridge`模式的工作原理很相似，父接口作为交换机转发子接口的数据。同一个网路的子接口可以通过父接口转发数据，如果想发送到其它网络，则报文会通过父接口的路由转发出去。

   2. `L3`模式

      - `L3`模式下，`IPvlan`有点像路由器的功能。
      - `IPvlan`在各个虚拟网络和主机网络之间进行不同网络报文的路由转发工作
      - 只要父接口相同，即使虚拟机/容器不在同一个网络，也可以互相`ping`通对方，因为`IPvlan`会在中间做报文转发工作

3. 测试`IPvlan`

   ```shell
   # 创建测试用的network namespace
   ip netns add net1
   ip netns add net2
   
   # 创建L3模式的虚拟网卡接口
   ip link add ipv1 link ens33 type ipvlan mode l3
   ip link add ipv2 link ens33 type ipvlan mode l3
   
   # 将创建的IPvlan接口放到namespace中
   ip link set ipv1 netns net1
   ip link set ipv2 netns net2
   ip netns exec net1 ip link set ipv1 up
   ip netns exec net2 ip link set ipv2 up
   
   #给两个虚拟接口配置不同网络的IP地址，并配置好路由项
   ip netns exec net1 ip addr add 10.0.1.10/24 dev ipv1
   ip netns exec net2 ip addr add 192.168.1.10/24 dev ipv2
   ip netns exec net1 ip route add default dev ipv1
   ip netns exec net2 ip route add default dev ipv2
   
   #测试两个网络的联通性
   ip netns exec net1 ping -c 3 192.168.1.10
   ```

4. `Docker IPvlan`网络

   ```shell
   # 此实验需要Docker 1.13+   Linux Kernel 3.19+
   
   # 创建IPvlan的网络
   docker network create -d ipvlan \
   	--subnet=192.168.30.0/24 \
   	-o parent=eth0 \
   	-o ipvlan_mode=l3 ipvlan30
   
   # 启动两个容器，发现在同一个IPvlan网络中的两个容器可以互相ping通
   docker run -it --net=ipvlan30 --name ipvlan_test1 --rm  busybox:latest /bin/sh
   docker run -it --net=ipvlan30 --name ipvlan_test2 --rm  busybox:latest /bin/sh
   
   # 在创建另外一个网络，和前面的网络不在同一个广播域
   docker network create -d ipvlan \
   	--subnet=192.168.30.0/24 \
   	-o parent=eth0 \
   	-o ipvlan_mode=l3 ipvlan110
   # 创建测试容器，发现可以ping通ipvlan30中的容器	
   docker run -it --net=ipvlan110 --name ipvlan_test3 --rm  busybox:latest /bin/sh
   ```

   

