一、`flannel`网络插件

1. 概述：

   - 集群内的所有主机使用一个网络，然后在每个主机上从该网络中划分出一个子网，`flannel`为主机上的容器创建网络时，从子网中划分一个`IP`给容器。
   - 容器跨节点访问需要解决的问题：
     - 容器`IP`地址重复问题：`flannel`设计了一种全局的网络地址分配机制，即使用`etcd`存储网段和节点之间的关系，然后`flannel`配置各个节点上的`Docker`，只在分配到当前节点的网段里选择容器`IP`地址。
     - 容器`IP`地址路由问题
       - `overlay`网络：`flannel`提供了`UDP`和`VXLAN`两种方法
       - `Host-Gateway`：该模式只能用于二层直接可达的网络，一般规模较小。

2. `flannel`简介

   1. `flannel`架构
      - `flannel`在架构上分为管理面和数据面，管理面主要包含一个`etcd`，用于协调各个节点上容器分配的网段，数据面即在每个节点上运行一个`flanneld`进程。
      - 集群内的所有`flannel`节点共享一个大的容器地址段，`flannel`一启动便会观察`etcd`，从`etcd`得知其他节点上容器已经占用的网段信息，然后向`etcd`申请该节点可用的`IP`地址段，并把该网段和主机`IP`地址信息都记录在`etcd`中。
      - `flannel`通过`etcd`分配了每个节点可用的`IP`地址段后，修改了`Docker`的启动参数，以确保每个节点上的`Docker`会使用不同的`IP`地址段。
      - `flannel`会观察`etcd`的数据，因此在其它节点向`etcd`更新网段和宿主机`IP`信息时，`etcd`就感知到了，在向其它主机上的容器转发网络包时，用对方容器所在主机的`IP`进行封包，然后将数据发往对应主机上的`flanneld`，再交由其转发给目的容器。
   2. `flannel`支持的底层实现
      - `UDP`
      - `VXLAN`
      - `Alloc`
      - `Host-Gateway`
      - `AWS VPC`
      - `GCE`路由

3. `flannel`安装配置

   1. `flanneld`启动时主要做了以下几个动作

      - 从`etcd`中获取`network`(大网)的配置信息
      - 划分`subnet`，并在`etcd`中注册
      - 将子网信息记录到`flannel`维护的`/run/flannel/subnet.env`文件中
      - 将`subnet.env`转写成一个`Docker`的环境变量文件`/run/flannel/docler`

   2. 由于`flannel`没有`Master`和`Slave`之分，`kubernetes`以`DaemonSet`方式部署`flannel`以达到每一个节点都部署一个`flanneld`实例的目的

   3. `flanneld`的配置文件以`ConfigMap`的形式挂载到容器内的`/etc/kube-flannel/`目录，供`flanneld`使用。

      ```yaml
      [root@master ~]# kubectl get cm -n kube-system -o yaml kube-flannel-cfg
      apiVersion: v1
      data:
        cni-conf.json: |
          {
            "name": "cbr0",
            "cniVersion": "0.3.1",
            "plugins": [
              {
                "type": "flannel",
                "delegate": {
                  "hairpinMode": true,
                  "isDefaultGateway": true
                }
              },
              {
                "type": "portmap",
                "capabilities": {
                  "portMappings": true
                }
              }
            ]
          }
        net-conf.json: |
          {
            "Network": "10.244.0.0/16",
            "Backend": {
              "Type": "vxlan"
            }
          }
      kind: ConfigMap
      metadata:
        ...
        labels:
          app: flannel
          tier: node
        name: kube-flannel-cfg
        namespace: kube-system
        resourceVersion: "1240"
      ```

4. `flannel backend`详解

   1. `UDP`
      - 当采用`UDP`模式时，`flanneld`进程在启动时会通过打开`/dev/net/tun`的方式生成一个`tun`设备
      - `flannel UDP`模式，同一主机之间通过`cni0`桥进行通信
      - 对于跨主机通信：
        - 对于发往别的主机的报文，查询路由表后发往`flannel0`设备，`flannel0`为`tun`设备，发送给`flannel0`的报文被`flanneld`进程接收。
        - `flanneld`进程查询`etcd`后，封装报文，发送到目的节点的`flanneld`进程
        - 目的节点解封报文后路由到本地节点的`cni0`网桥，而后转发给对应容器。
      - `flanneld`进程的主要作用
        - `UDP`封包解包
        - 根据`etcd`的数据刷新本节点路由表
      - 缺点：
        - 网络数据包先通过`tun`设备从内核中复制到用户态，用户态处理完后再发送到内核态，效率很低
   2. `VXLAN`
      - `flanneld`启动时先确定`VTEP`设备(`flannel.1`)已存在，若已经创建则跳过，并将`VTEP`的信息上报到`etcd`中，当在`flannel`网络中有新的节点加入集群并向`etcd`注册时，各个节点上的`flanneld`从`etcd`中得到通知，并以此执行以下流程
        - 在节点中创建一条改系欸但所属网段的路由表，主要是能让`Pod`中的流量路由到`flannel.1`接口
        - 在节点中添加一条该节点的`IP`及`VTEP`设备的静态`ARP`缓存。
      - 在`VXLAN`模式下，数据是由内核转发的，`flannel`不转发数据，仅动态设置`ARP`和`FDB`表项。
   3. `Host Gateway`
      - 通过把主机当作网关实现跨接点通信
      - `flanneld`的唯一作用就是负责主机上路由表的动态刷新

5. `flannel`与`etcd`

   `flannel`需要使用`etcd`存储网络元数据，存储路径可通过以下方式获得

   `# etcdctl get "" --prefix --key-only | grep -Ev "^$" |grep "flannel"`

二、全能大三层网络插件：`Calico`

三、`Weave`：支持数据加密的网络插件

1. 简介
   - `Weave`是一个多主机容器网络方案，能够创建一个虚拟网络，用于连接部署在多台主机上的`Docker`容器，这样容器就像被接入了一个网络交换机。
   - `Weave`支持将容器应用暴露成服务公外部访问。
   - `Weave`的控制平面是个去中心化的架构，`Weave`在网络中的节点通过`Gossip`协议进行数据同步。
   - `Weave`网络中的每个主机都会安装`wRouter`，这些`wRouter`之间建立全网络的`TCP`连接，通过这个连接进行心跳握手和拓扑信息交换，以维护可用网络的最新视图。
2. 在`Kubernetes`中部署`Weave`
   - `https://github.com/weaveworks/weave/releases`
   - `wget https://github.com/weaveworks/weave/releases/download/v2.6.2/weave-daemonset-k8s-1.11.yaml`
   - `kubectl apply -f  weave-daemonset-k8s-1.11.yaml`
3. `Weave`实现原理
   - 数据平面上，`Weave`的封包实现了`L2 overlay`，封包支持两种模式
     - `Sleeve`模式：运行在用户态，类似于`flannel`的`UDP`模式
     - `fasepath(快速数据路径)`模式：通过`VXLAN`封包
   - `Weave`独有的功能是对整个网络的简单加密
   - `Weave`网络主要通过路由服务和网桥转发，`Weave`会在主机上创建一个网桥，每一个容器通过`veth pair`连接到该网桥，容器的`IP`地址可以由用户或者`Weave`的`IPADM`分配。同时，网桥上有个`wRouter`容器与之相连，该`Router`会通过连接在网桥上的接口抓取网络包，`Weave`网桥工作在`Promisc`模式。

四、`Cilium`：为微服务网络连接安全而生

五、`CNI-Genie`

1. `CNI-Genie`本质上就是`Kubernetes`和底层多个`CNI`插件之间的适配器(`adapter`)

2. `CNI-Genie`通过`Kubernetes Pod`的`annotation`指定其要使用的底层`CNI`插件

3. 容器多`IP`

   ```yaml
   annotations:
     cni: "calico,weave"
     multi-ip-preferences: |
     {
       "multi_entry": 0,
       "ips": {
         ""{
           "ip": "",
           "interface": ""
         }
       }
     }
   # 指定了两个网络接口，分别由calico和weave提供  
   ```

   

