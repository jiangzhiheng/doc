一、`Kubernetes`监控方案

1. `Heapster`：
   - 对于`Kubernetes`需要考虑的监控项：
     - `Kubernetes`节点，比如节点的`CPU,Load,Disk,Memory`等指标
     - 内部系统组件的状态，比如`kube-scheduler,kube-controller-manager,Kube-DNS/CoreDNS`等
     - 编排级的`metrics`，比如`Deployment,Pod,DaemonSet,StatefulSet`等资源的状态，资源请求，调度和`API`延迟等数据指标
2. `kube-state-metrics`
   - `kube-state-metrics`通过监听`API Server`生成有关资源对象的状态指标，比如：
     - `Deployment`调度了多少个`Pod`副本，现在可用的有几个？
     - 有多少个`Pod`是`running,stopped`或`terminated`状态？
     - `Pod`重启了几次？
3. `kube-state-metrics`和`metric-server`的区别
   - `kube-state-metrics`主要关注集群资源相关的一些元数据，比如`Deployment,Pod`副本状态和资源限额等静态指标。
   - `metric-server`主要关注资源度量`API`的实现，比如`CPU`，文件描述符，内存，请求延时等实时指标。
4. `Prometheus`

二、`Prometheus`的安装配置

1. 基本安装（`k8s`环境）

   1. 创建`namespace`

      `kubectl create ns kube-ops`

   2. 创建`configmap`

   3. 创建`deploy`资源

   4. 定义`ServiceAccount`以及`RBAC`授权

   5. 定义`Service`资源

   `https://github.com/jiangzhiheng/k8sDemo/tree/master/prometheus/simple_install`

2. 安装`Prometheus Operator`

   1. 概述
      - `Operator`是由`CoreOS`公司开发的用来扩展`Kubernetes API`的特定应用程序控制器，用来创建，配置和管理复杂的有状态应用，例如数据库，缓存和监控系统，`Prometheus Operator`就是基于`Operator`框架开发的管理`Prometheus`集群的控制器
      - `Prometheus Operator`架构：
        - `Operator`是核心部分，作为一个控制器而存在，`Operator`会创建`Prometheus,ServiceMonitor,AlertManager及PronetheusRule`这四个`CRD`资源对象，然后一直监控并维持这四个资源对象的状态
        - `Prometheus`资源对象是作为`Prometheus Server`存在的
        - `ServiceMonitor`资源对象是专门提供`Metrics`数据接口的`exporter`的抽象
        - `Alertmanager`资源对象是对应`AlertManager`组件的抽象
        - `PrometheusRule`资源对象是被`Prometheus`实例使用的告警规则文件的抽象
      
   2. 手动部署`Prometheus Operator`
      - `git clone https://gitee.com/jzh_k/kube-prometheus.git`
      - `kubectl create -f manifests/setup`
      - `kubectl create -f manifests/`
      - 配置`grafana`：如果想要安装插件，需要配置`grafana-deployment.yaml`插件存储路径，默认为`emptyDir`
      
   3. 配置`kube-scheduler`和`kube-controller-manager`的监控目标

      - `prometheus-serviceMonitorKubeScheduler.yaml`

        ```yaml
        apiVersion: monitoring.coreos.com/v1
        kind: ServiceMonitor
        metadata:
          labels:
            k8s-app: kube-scheduler
          name: kube-scheduler
          namespace: monitoring
        spec:
          endpoints:
          - interval: 30s   # 30s获取一次信息
            port: http-metrics  # 对应的Service的端口名
          jobLabel: k8s-app
          namespaceSelector:   #表示匹配某名称空间中的service
            matchNames:
            - kube-system
          selector:  # 匹配service
            matchLabels:
              k8s-app: kube-scheduler
        ```

      - `prometheus-kubeSchedulerService.yaml`

        ```yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: kube-scheduler
          namespace: kube-system
          labels:
            k8s-app: kube-scheduler
        spec:
          selector:
            component: kube-scheduler
          ports:
          - name: http-metrics
            port: 10251
            targetPort: 10251
            protocol: TCP
        ```

      - `kubectl apply -f prometheus-kubeSchedulerService.yaml`

3. 在`Prometheus`中添加自定义的监控项

   1. 概述：

      - 首先，建立一个`ServiceMonitor`对象，用于为`Proemtheus`添加监控项
      - 然后，将`ServiceMonitor`对象关联`Metrics`数据接口的一个`Service`对象
      - `Service`对象可以正确获取`Metrics`数据，以下为以配置`Etcd`集群监控为例

   2. 配置步骤

      1. 配置`etcd`证书

         ```shell
         # 查看etcd的livenessProbe
         # 如果需要使用https认证，则将认证使用到的证书通过secret对象保存到集群中
         kubectl create secret generic etcd-certs \
           --from-file=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
           --from-file=/etc/kubernetes/pki/etcd/healthcheck-client.key \
           --from-file=/etc/kubernetes/pki/etcd/ca.key \
           -n monitoring 
         ```
         
         ```shell
            # 将创建的etcd-cert对象配置到prometheus资源对象中
            [root@master ~]# kubectl get prometheus -n monitoring
            NAME   VERSION   REPLICAS   AGE
            k8s    v2.15.2   2          45m
            [root@master ~]# kubectl edit prometheus k8s -n monitoring
              ...
              podMonitorSelector: {}
              replicas: 2
              secrets:
              - etcd-cert
              ...
         ```
         
         
      
   2. 创建`ServiceMonitor`
      
         ```yaml
         apiVersion: monitoring.coreos.com/v1
         kind: ServiceMonitor
         metadata:
           name: etcd-k8s
           namespace: monitoring
           labels:
             k8s-app: etcd-k8s
         spec:
           jobLabel: k8s-app
           endpoints:
           - port: port
             interval: 30s
             scheme: https
             tlsConfig:
               caFile: /etc/prometheus/secrets/etcd-certs/ca.crt
               certFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.crt
               keyFile: /etc/prometheus/secrets/etcd-certs/healthcheck-client.key
               insecureSkipVerify: true
           selector:
             matchLabels:
               k8s-app: etcd
           namespaceSelector:
             matchNames:
             - kube-system
      ```
      
   3. 创建`Service`
      
         ```yaml
         # etcd集群一般位于集群外，需要手动创建一个Endpoints
         apiVersion: v1
         kind: Service
         metadata:
           name: etcd-k8s
           namespace: kube-system
           labels:
             k8s-app: etcd
         spec:
           type: ClusterIP
           clusterIP: None
           ports:
           - name: port
             port: 2379
             protocol: TCP
         ---
         apiVersion: v1
         kind: Endpoints
         metadata:
           name: etcd-k8s
           namespace: kube-system
           labels:
             k8s-app: etcd
         subsets:
         - addresses:
           - ip: 192.168.1.102
             nodeName: etcd-master
           ports:
           - name: port
             port: 2379
             protocol: TCP
         ```

4. 在`Prometheus Operator`中添加自定义告警

   1. 概述

      - 对`AlertManagers`实例的配置，是通过`role`为`endpoints`的`Kubernetes`的服务发现机制获取的，匹配的服务名为`alertmanager-main`且端口名为`web`的`Service`
      - 对应的告警规则文件位于`/etc/prometheus/rules/prometheus-k8s-rulefiles-0/`目录下所有的`YAML`文件中。

   2. `PrometheusRule`定义示例

      ```yaml
      apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      metadata:
        labels:
          prometheus: k8s
          role: alert-rules
        name: prometheus-k8s-rules
        namespace: monitoring
      spec:
        groups:
        - name: node-exporter.rules
          rules:
          - expr: |
              count without (cpu) (
                count without (mode) (
                  node_cpu_seconds_total{job="node-exporter"}
                )
              )
            record: instance:node_num_cpu:sum
          - expr: |
              1 - avg without (cpu, mode) (
                rate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[1m])
              )
            record: instance:node_cpu_utilisation:rate1m
      ```

   3. 注意：

      - 如果想要自定义一个告警规则，则只需要创建一个具有`prometheus=k8s`和`role=alert-rules`标签的`PrometheusRule`对象

   4. 配置告警

      1. 将`alertmanager-main`这个`Service`改为`NodePort`类型的`Service`，之后可以在页面的`status`路径下看`AlterManager`的配置信息。

         ```yaml
         global:
           resolve_timeout: 5m
           http_config: {}
           smtp_hello: localhost
           smtp_require_tls: true
           pagerduty_url: https://events.pagerduty.com/v2/enqueue
           hipchat_api_url: https://api.hipchat.com/
           opsgenie_api_url: https://api.opsgenie.com/
           wechat_api_url: https://qyapi.weixin.qq.com/cgi-bin/
           victorops_api_url: https://alert.victorops.com/integrations/generic/20131114/alert/
         route:
           receiver: Default
           group_by:
           - namespace
           routes:
           - receiver: Watchdog
             match:
               alertname: Watchdog
           - receiver: Critical
             match:
               severity: critical
           group_wait: 30s
           group_interval: 5m
           repeat_interval: 12h
         ```

      2. 如果想要添加自己的接收器或者模板消息，就可以更改这个文件，例如：

         ```yaml
         global:
           resolve_timeout: 5m
           smtp_smarthost: 'smtp.163.com:25'
           smtp_from: 'jiangzh931225@163.com'
           smtp_auth_username: 'jiangzh931225@163.com'
           smtp_auth_password: 'TQPQAXLUVBSFOLNU'
           smtp_hello: '163.com'
           smtp_require_tls: false
         route:
           group_by: ['job','severity']
           group_wait: 30s
           group_interval: 5m
           repeat_interval: 12h
           receiver: Default
           routes:
           - receiver: webhook
             match:
               alertname: CoreDNSDown
         receivers:
         - name: 'default'
           email_configs:
           - to: '1689991551@qq.com'
             send_resolved: true
         - name: 'webhook'
           webhook_configs:
           - url: 'http://dingtalk-hook.kube-ops:5000'
           send_resolved: true  
         ```

         - 将上面的文件保存为`alertmanager.yaml`，然后使用这个文件创建一个`Secret`对象

           ```shell
           # 删除之前的Secret对象
           kubectl delete secret alertmanager-main -n monitoring
           # 创建新的secret对象
           kubectl create secret generic alertmanager-main --from-file=alertmanager.yaml -n monitoring
           ```

5. `Prometheus Operator`的高级配置（服务发现）

   目的：在`Prometheus Operator`中自动发现并监控具有`prometheus.io/scrape=true`这个`annotation`的`Service`

   1. `Prometheus`上`Kubernetes`服务配置文件

      ```yaml
      - job_name: kubernetes-apiservers
        scrape_interval: 1m
        scrape_timeout: 10s
        metrics_path: /metrics
        scheme: https
        kubernetes_sd_configs:
        - api_server: null
          role: endpoints
          namespaces:
            names: []
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          separator: ;
          regex: default;kubernetes;https
          replacement: $1
          action: keep
      - job_name: kubernetes-nodes
        scrape_interval: 1m
        scrape_timeout: 10s
        metrics_path: /metrics
        scheme: https
        kubernetes_sd_configs:
        - api_server: null
          role: node
          namespaces:
            names: []
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        relabel_configs:
        - separator: ;
          regex: __meta_kubernetes_node_label_(.+)
          replacement: $1
          action: labelmap
        - separator: ;
          regex: (.*)
          target_label: __address__
          replacement: kubernetes.default.svc:443
          action: replace
        - source_labels: [__meta_kubernetes_node_name]
          separator: ;
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics
          action: replace
      - job_name: kubernetes-cadvisor
        scrape_interval: 1m
        scrape_timeout: 10s
        metrics_path: /metrics
        scheme: https
        kubernetes_sd_configs:
        - api_server: null
          role: node
          namespaces:
            names: []
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: false
        relabel_configs:
        - separator: ;
          regex: __meta_kubernetes_node_label_(.+)
          replacement: $1
          action: labelmap
        - separator: ;
          regex: (.*)
          target_label: __address__
          replacement: kubernetes.default.svc:443
          action: replace
        - source_labels: [__meta_kubernetes_node_name]
          separator: ;
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          action: replace
      - job_name: kubernetes-service-endpoints
        scrape_interval: 1m
        scrape_timeout: 10s
        metrics_path: /metrics
        scheme: http
        kubernetes_sd_configs:
        - api_server: null
          role: endpoints
          namespaces:
            names: []
        relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          separator: ;
          regex: "true"
          replacement: $1
          action: keep
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
          separator: ;
          regex: (https?)
          target_label: __scheme__
          replacement: $1
          action: replace
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          separator: ;
          regex: (.+)
          target_label: __metrics_path__
          replacement: $1
          action: replace
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          separator: ;
          regex: ([^:]+)(?::\d+)?;(\d+)
          target_label: __address__
          replacement: $1:$2
          action: replace
        - separator: ;
          regex: __meta_kubernetes_service_label_(.+)
          replacement: $1
          action: labelmap
        - source_labels: [__meta_kubernetes_namespace]
          separator: ;
          regex: (.*)
          target_label: kubernetes_namespace
          replacement: $1
          action: replace
        - source_labels: [__meta_kubernetes_service_name]
          separator: ;
          regex: (.*)
          target_label: kubernetes_name
          replacement: $1
          action: replace
      ```

   2. 将上面的文件直接保存为`prometheus-additional.yaml`，然后通过这个文件创建一个对应的`secret`对象

      `kubectl create secret generic additional-configs --from-file=prometheus-additional.yaml -n monitoring`

   3. 修改`Prometheus`的资源对象文件（`prometheus-prometheus.yaml`文件）

      ```yaml
      additionalScrapeConfigs:
          name: additional-configs
          key: prometheus-additional.yaml
      ```

   4. 更新`ClusterRole`资源对象

      ```yaml
      apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      metadata:
        name: prometheus-k8s
      rules:
      - apiGroups: [""]
        resources:
        - configmaps
        - secrets
        - nodes
        - pods
        - nodes/proxy
        - services
        - resourcequotas
        - replicationcontrollers
        - limitranges
        - persistentvolumeclaims
        - persistentvolumes
        - namespaces
        - endpoints
        verbs: ["get", "list", "watch"]
      - apiGroups: ["extensions"]
        resources:
        - daemonsets
        - deployments
        - replicasets
        - ingresses
        verbs: ["get", "list", "watch"]
      - apiGroups: ["apps"]
        resources:
        - daemonsets
        - deployments
        - replicasets
        - statefulsets
        verbs: ["get", "list", "watch"]
      - apiGroups: ["batch"]
        resources:
        - cronjobs
        - jobs
        verbs: ["get", "list", "watch"]
      - apiGroups: ["autoscaling"]
        resources:
        - horizontalpodautoscalers
        verbs: ["get", "list", "watch"]
      - apiGroups: ["policy"]
        resources:
        - poddisruptionbudgets
        verbs: ["get", list", "watch"]
      - nonResourceURLs: ["/metrics"]
        verbs: ["get"]
      ```

      `https://www.jianshu.com/p/407484e16a95`

   5. 数据持久化配置