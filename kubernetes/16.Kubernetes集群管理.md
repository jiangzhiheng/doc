一、`Node`的管理

1. `Node`的隔离与恢复

   - 在硬件升级或硬件维护情况，我们需要将`Node`隔离，使其脱离集群的调度范围

     - 方式一：

       ```yaml
       apiVersion: v1
       kind: Node
       metadata:
         name: node01.jzh.com
         labels:
           kubernetes.io/hostname: node01.jzh.com
       spec:
         unschedulable: true
       # 通过kubectl replace完成对Node状态的修改
       # kubectl replace -f unschedule_node.yaml
       ```

     - 方式二：

       `kubectl patch  node  node01.jzh.com -p '{"spec":{"unschedulable":true}}'`

       注意：将某个`Node`脱离调度范围时，在其上运行的`Pod`并不会自动停止，需要手动停止(`kubectl drain  node01.jzh.com --force`)

     - 方式三

       `kubectl cordon node01.jzh.com`

       `kubectl uncordon node01.jzh.com`

2. `Node`的扩容

二、更新资源对象的`Label`

- 给已创建的`Pod`添加一个标签

  `kubectl label pod redis-master-pod role=backend`

- 删除一个`Label`，`Label`的`key`后加一个减号即可

  `kubectl label pod redis-master-pod role-`

- 修改一个已存在的`label`,需要加`--overwrite`参数

  `kubectl label pod redis-master-pod role=master --overwrite`

三、`Namespace`：集群环境共享与隔离

1. 创建名称空间

   `kubectl create namespace prod`

2. 定义`Context`（运行空间）

   ```shell
   # 定义ns
   kubectl create ns prod
   kubectl create ns dev
   # 定义context
   kubectl config set-context ctx-dev --namespace=dev --cluster=kubernetes --user=dev
   kubectl config set-context ctx-prod --namespace=prod --cluster=kubernetes --user=prod
   ```

3. 设置工作组在特定的`Context`环境下工作

   - 设置运行环境

     `kubectl config use-context ctx-dev`

   - 在该环境下创建的`Pod`默认会在`dev`名称空间下。

四、`Kubernetes`资源管理

1. 计算资源管理

   - `Request`和`limit`参数
   - 基于`Request`和`Limit`的调度机制
   - 对大页内存的支持

2. 资源配置范围管理（`LimitRange`）

   - 创建一个`Namespace`

     `kubectl create namespace limit-example`

   - 为`Namespace`设置`LimitRange`

     ```yaml
     apiVersion: v1
     kind: LimitRange
     metadata:
       name: mylimits
     spec:
       limits:
       - type: Pod
         max:
           cpu: "4"
           memory: 2Gi
         min:
           cpu: 200m
           memory: 6Mi
         maxLimitRequestRatio:
           cpu: 3
           memory: 2
       - type: Container
         default:
           cpu: 300m
           memory: 200Mi
         defaultRequest:
           cpu: 200m
           memory: 100Mi
         max:
           cpu: "2"
           memory: 1Gi
         min:
           cpu: 100m
           memory: 3Mi
         maxLimitRequestRatio:
           cpu: 5
           memory: 4
     # kubectl create -f limitrange.yaml --namespace=limit-example
     [root@master clusterMgr]# kubectl get limits --namespace=limit-example
     NAME       CREATED AT
     mylimits   2020-04-15T10:26:12Z
     [root@master clusterMgr]# kubectl describe limits mylimits --namespace=limit-example
     Name:       mylimits
     Namespace:  default
     Type        Resource  Min   Max  Default Request  Default Limit  Max Limit/Request Ratio
     ----        --------  ---   ---  ---------------  -------------  -----------------------
     Pod         cpu       200m  4    -                -              3
     Pod         memory    6Mi   2Gi  -                -              2
     Container   cpu       100m  2    200m             200m           5
     Container   memory    3Mi   1Gi  100Mi            200Mi          4
     
     ```

     - 不论是`CPU`还是内存，在`LimitRange`中，`Pod`和`Container`都可以设置`Min,Max和Max Limit/Request Ratio`参数，`Container`还可设置`Default Request`和`Default Limit`
     - `Limit/Request Ratio`参数限制了`Pod`中所有容器的`Limits`值与`Request`值的比例上限。
     - 如果设置了`Container`的`Max`，那么对于该类资源而言，整个集群中的所有容器都必须设置`Limits`，否则无法成功创建。`Pod`内的容器未配置`limits`时，将使用`Default Limit`的值。
     - 如果设置了`Container`的`Min`，那么对于该类资源而言，整个集群中的所有容器都必须设置`Request`。
     - `Pod`内的任何容器的`Limit`与`Requests`的比例都不能超过`Container`的`Limit/Request Ratio`

3. 资源服务质量管理（`Resource QoS`）

   - `Kubernetes`中`Pod`的`Requests`和`Limits`资源配置有如下特点
     - 如果`Pod`配置的`Request`值等于`Limit`值，那么该`Pod`获得的资源是完全可靠的。
     - 如果`Pod`的`Request`值小于`Limit`值，那么该`Pod`获得的资源可分为两部分：
       - 完全可靠的值，资源量大小等于`Requests`值。
       - 不可靠的资源，资源量的大小等于`Limits`与`Requests`的差额。
   - 服务质量等级：
     - `Guaranteed`：`Pod`中的容器对所有资源类型都定义了`Limits`和`Requests`，并且所有容器的`Limits`值都和`Requests`相等，这种情况下，容器可以不定义`Requests`，只定义`limits`。
     - `BestEfford`：`Pod`中所有容器都未定义资源配置。
     - `Burstable`：介于以上两种之间。
   - `Kubernetes QoS`的工作特点
   - `OOM`计分系统

4. 资源配额管理（`Resource Quotas`）

   1. 特点：

      - 通过`Resource Quota`对象，我们可以定义资源配额，这个资源配额可以为每个命名空间都提供一个总体的资源使用的限制，他可以限制命名空间中某种类型的对象的总数目上限，也可以设置命名空间中`Pod`可以使用的计算资源上限。
      - 如果命名空间中计算资源的资源配额启用，那么用户必须为相应的资源类型设置`Requests`或`limits`；否则配额系统可能会直接拒绝`Pod`的创建。

   2. 在`Master`中开启资源配额选型

      - 概述：
        - 资源配额可以通过在`kube-apiserver`的`--admission-control`参数值中添加`ResourceQuota`参数进行开启
        - 如果在某个命名空间的定义中存在`ResourceQuota`，那么对于该命名空间而言，资源配额就是开启的。
        - 一个命名空间中可以有多个`ResourceQuota`配置项
      - 计算资源配额（`Compute Resource Quota`）
      - 存储资源配额（`Volume Count Quota`）
      - 对象数量配额（`Object Count Quota`）

   3. 配额的作用域（`Quota Scopes`）

      - `Terminating`
      - `NotTerminating`
      - `BestEffort`
      - `NotBestEffort`

   4. 在资源配额中设置`Requests`和`Limits`：如果在资源配额中指定了`requests.cpu`或`requests.memory`，那么它会强制要求每个容器都配置自己的`CPU Requests`或`CPU Limit`

   5. 资源配额的定义

      1. 创建名称空间

         `kubectl create ns myspace`

      2. 计算资源的配额示例

         ```yaml
         apiVersion: v1
         kind: ResourceQuota
         metadata:
           name: compute-resource-quota
         spec:
           hard:
             pods: "4"
             requests.cpu: "1"
             requests.memory: 1Gi
             limits.cpu: "2"
             limits.memory: 2Gi
         # kubectl apply -f compute-resource-quota.yaml -n myspace    
         ```

      3. 对象资源的配额示例

         ```yaml
         apiVersion: v1
         kind: ResourceQuota
         kind:
           name: object-count-quota
         spec:
           hard:
             configmaps: "10"
             persistentvolumeclaims: "4"
             replicationcontrollers: "20"
             secrets: "10"
             services: "10"
             services.loadbalancers: "2"
         # kubectl apply -f object-counts.yaml -n myspace
         ```

         ```shell
         [root@master ~]# kubectl get quota -n myspace
         NAME                     CREATED AT
         compute-resource-quota   2020-04-17T02:58:03Z
         object-count-quota       2020-04-17T03:06:00Z
         [root@master ~]# kubectl describe quota compute-resource-quota -n myspace
         Name:            compute-resource-quota
         Namespace:       myspace
         Resource         Used  Hard
         --------         ----  ----
         limits.cpu       0     2
         limits.memory    0     2Gi
         pods             0     4
         requests.cpu     0     1
         requests.memory  0     1Gi
         [root@master ~]# kubectl describe quota object-count-quota -n myspace
         Name:                   object-count-quota
         Namespace:              myspace
         Resource                Used  Hard
         --------                ----  ----
         configmaps              0     10
         persistentvolumeclaims  0     4
         replicationcontrollers  0     20
         secrets                 1     10
         services                0     10
         services.loadbalancers  0     2
         ```

   6. 资源配额与集群资源总量的关系：

      - 资源配额将整个集群中的资源总量做了一个静态划分，但它并没有对集群中的节点做任何限制：不同命名空间中的 `Pod`任然可以运行在同一个节点上。
      - 资源配额与集群资源总量是完全独立的。

5. `ResourceQuota`和`LimitRange`实践

   1. 需要实现功能如下：
      - 限制运行状态`Pod`的计算资源用量。
      - 限制持久存储卷的数量以控制对存储的访问
      - 限制负载均衡器的数量以控制成本
      - 防止滥用网络端口这类稀缺资源
      - 提供默认的计算资源`Requests`以便于系统做出更优化的调度
   2. 创建名称空间
   3. 设定资源配额

五、资源紧缺时的`Pod`驱逐机制

1. 驱逐策略

2. 驱逐信号

3. 驱逐阈值

4. 驱逐监控频率

   `--housekeeping-interval`

5. 节点的状况

6. 节点状况的抖动

   `--eviction-pressure-transition-period`

7. 回收`node`级别的资源

8. 驱逐用户的`Pod`

9. 资源最少回收量

   `--eviction-minimum-reclaim="memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi"`

10. 节点资源紧缺情况下的系统行为

六、`Pod Disruption Budget`（主动驱逐保护）

1. 使用场景

   - 节点的维护或升级时（`kubectl drain`）
   - 对应用的自动缩容操作（`autoscaling down`）

2. 主要字段

   - `Label Selector`：用于筛选被管理的`Pod`
   - `minAvailable`：指定驱逐过程需要保障的最少`Pod`数量。`minAvailable`可以是一个数字，也可以是一个百分比，例如100%就表示不允许进行主动驱逐

3. 定义示例

   ```yaml
   apiVersion: policy/v1beta1
   kind: PodDisruptionBudget
   metadata:
     name: nginx-prod
   spec:
     minAvailable: 3  # 设置存活Pod的数量不得少于3个
     selector:
       matchLabels:
         name: nginx
   ```

   