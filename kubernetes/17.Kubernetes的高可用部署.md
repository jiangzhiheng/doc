一、手动部署的高可用方案

1. `etcd`的高可用部署

   1. 配置要点：

      - `etcd`需要以集群的方式进行部署，以实现`etcd`数据存储的冗余，备份和高可用
      - `etcd`存储的数据本身也应考虑使用可靠的存储设备。

   2. 规划要点：

      - `etcd`集群的部署可以使用静态配置，也可使用`etcd`提供的基于`REST API`在运行时动态添加、修改或删除集群中的成员
      - 节点数量推荐至少为3个

   3. 配置文件示例：

      `/etc/etcd/etcd.conf`

      ```shell
      # [member]
      ETCD_NAME=etcd1   # ETCD实例的名称
      ETCD_DATA_DIR="/var/lib/etcd"   # ETCD数据保存目录
      ETCD_LISTEN_CLIENT_URLS="http://10.0.0.1:2379,http://127.0.0.1:2379"  # 供外部客户端使用的URL
      ETCD_ADVERTISE_CLIENT_URLS="http://10.0.0.1:2379,http://127.0.0.1:2379" #s 广播给外部客户端使用的URL
      # [cluster]
      ETCD_LISTEN_PEER_URLS="http://10.0.0.1:2380"  # 集群内部通信使用的URL
      ETCD_INITIAL_ADVERTISE_PEER_URLS="http://10.0.0.1:2380"  # 广播给集群内其它成员访问的URL
      ETCD_INITIAL_CLUSTER="etcd1=http://10.0.0.1:2380,etcd2=http://10.0.0.2:2380,etcd3=http://10.0.0.3:2380"  # 初始集群成员列表
      ETCD_INITIAL_CLUSTER_STATE="new"  # 初始集群状态，new为新建集群
      ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster" #集群名称
      ```

      - 其它两个节点配置文件相同，只需修改对应节点地址即可

   4. 启功三个节点上的`etcd`服务

      `systemctl restart etcd`

   5. 验证集群状态：

      `etcdctl cluster-health`

      `etcdctl member list`

   6. 参考文档：`https://etcd.io/docs/v3.4.0/op-guide/clustering/`

2. `Master`的高可用部署

   1. 配置要点：
      - `Kubernetes`建议`Master`的3个组件都已容器的形式启动，启动它们的基础工具是`kubelet`，所以他们都将以`Static Pod`的形式启动并将由`kubelet`监控和自动重启。
      - `kubelet`本身的高可用则通过操作系统来完成，例如使用`Linux`的`Systemd`系统进行管理
   2. `kube-apiserver`的高可用部署
      1. 假设`kubelet`的启动参数指定`--config=/etc/kubernetes/manifests`，即`Static Pod`定义文件所在的目录，接下来就可以创建`kube-apiserver.yaml`配置文件用于启动`kube-apiserver`了
      2. `YAML`文件示例：权威指南705页
      3. 配置要点：
         - `kube-apiserver`需要使用`hostNetwork`模式，即直接使用宿主机网络，可以使客户端能够通过物理机访问其`API`
         - 端口号的设置都配置了`hostPort`，将容器内的端口号直接映射为宿主机的端口号
         - 为三个节点执行重复的操作，使得在每台服务器上都启动一个`kube-apiserver`的`Pod`
      4. 为`kube-apiserver`配置负载均衡器
         - 在不同的平台下，负载均衡的实现方式不同：
           - 在一些公有云如`GCE、AWS`、阿里云上都有现成的实现方案
           - 对于本地集群，我们可以选择硬件或者软件来实现负载均衡，`Kubernetes`社区推荐的方案`HAProxy`和`Keepalived`，其中`HAProxy`负责负载均衡，`Keepalived`负责对`HAProxy`进行监控和故障切换
         - 配置要点：
           - 如果`Master`开启了安全认证机制，那么需要确保在`CA`证书中包含负载均衡服务节点的`IP`
           - 对于外部的访问，比如通过`kubectl`访问`API Server`，需要配置为访问`API Server`对应的负载均衡器的`IP`地址
   3. `kube-controller-manager`和`kube-scheduler`的高可用配置
      1. 概述
         - 不同于`API Server`，`Master`另外两个组件`kube-controller-manager`和`kube-scheduler`会修改集群的状态信息
         - 因此，对于`kube-controller-manager`和`kube-scheduler`而言，高可用不仅意味着需要启动多个实例，还需要这些个实例能实现选举并选举出`leader`，以保证同一时间只有一个实例可以对集群状态信息进行读写，避免痴线同步问题和一致性问题
         - `Kubernetes`对于这种选举机制的实现是通过租赁锁（`lease-lock`）来实现的，我们可以通过在这两个组件的启动参数中设置`--leader-elect=true`，来保证同一时间只会运行一个可修改集群信息的实例。
      2. 具体实现步骤
         - 在每个`Master`上都创建对应的日志文件
         - 创建`kube-controller-manager`和`kube-scheduler`的`YAML`配置文件
         - 将这两个文件复制到`kubelet`监控的`/etc/kubernetes/manifests`目录下。

二、使用`kubeadm`的高可用部署方案

1. `kubeadm`提供了两种不同的高可用方案

   - 堆叠方案：`etcd`服务和控制平面被部署在同样的节点中，对基础设施的要求较低，对故障的应对能力也较低
   - 外置`etcd`方案：`etcd`和控制平面被分离，需要更多的硬件，也有更好的保障能力

2. 安装步骤（简单示例）

   1. 准备工作：`Master`服务器至少3台，外置`etcd`的方案则需要有3台以上的`etcd`服务器

   2. 为`API Server`提供负载均衡服务

      ```shell
      yum -y install haproxy
      vim /etc/haproxy/haproxy.cfg
      ```

      ```ini
      # 配置文件中添加：
      # 开放12345端口提供HAProxy的状态服务，用于观察服务状态
      listen stats 0.0.0.0:12345
              mode    http
              log     global
              maxconn 10
              stats   enable
              stats   hide-version
              stats   refresh 30s
              stats   show-node
              stats   auth    admin:p@ssw0rd
              stats   uri     /stats
      # 定义HAProxy的12567端口提供代理服务        
      frontend kube-api-https   
              bind    0.0.0.0:12567
              mode    tcp
              default_backend kube-api-server
      backend kube-api-server
              balance roundrobin
              mode    tcp
              server  kubenode1 192.168.1.145:6443 check
              server  kubenode2 192.168.1.146:6443 check
              server  kubenode3 192.168.1.147:6443 check
      ```

      ```shell
      systemctl enable haproxy
      systemctl start haproxy
      # 浏览器打开http://192.168.1.145:12345/stats
      ```

   3. 启动第一组控制平面

      1. 创建初始化配置文件

         ```shell
         kubeadm config print init-defaults > init.defaule.yaml
         ```

      2. 修改初始化配置文件

         ```yaml
         apiVersion: kubeadm.k8s.io/v1beta2
         bootstrapTokens:
         - groups:
           - system:bootstrappers:kubeadm:default-node-token
           token: abcdef.0123456789abcdef
           ttl: 24h0m0s
           usages:
           - signing
           - authentication
         kind: InitConfiguration
         localAPIEndpoint:
           advertiseAddress: 192.168.1.145  # 当前节点监听的地址
           bindPort: 6443
         nodeRegistration:
           criSocket: /var/run/dockershim.sock
           name: master
           taints:
           - effect: NoSchedule
             key: node-role.kubernetes.io/master
         ---
         apiServer:
           timeoutForControlPlane: 4m0s
         apiVersion: kubeadm.k8s.io/v1beta2
         certificatesDir: /etc/kubernetes/pki
         clusterName: mycluster
         controllerManager: {}
         dns:
           type: CoreDNS
         etcd:
           local:
             dataDir: /var/lib/etcd
         imageRepository: registry.cn-shenzhen.aliyuncs.com/jzh  # 修改镜像源
         kind: ClusterConfiguration
         kubernetesVersion: v1.17.3
         controlPlaneEndpoint: "192.168.1.145:12567"   # HAProxy提供的代理端口
         networking:
           dnsDomain: cluster.local
           serviceSubnet: 10.96.0.0/12
           podSubnet: 10.244.0.0/16
         scheduler: {}
         ```

      3. 初始化控制平面

         ```shell
         kubeadm init --config=init.default.yaml --upload-certs
         
         # --upload-certs参数专用于高可用部署，可以将需要在不同的控制平面之间传递的证书文件上传到集群中，以Secret形式保存起来，并使用Token加密，这个Secret两小时后会过期，kubeadm init phase upload-certs --upload-certs命令可重新生成
         
         
         # 输出内容：
         ...
         You can now join any number of the control-plane node running the following command on each as root:
         # 加入新的控制平面的命令
           kubeadm join 192.168.1.145:12567 --token abcdef.0123456789abcdef \
             --discovery-token-ca-cert-hash sha256:9463bec075db3d70e659c43d578e7658f2e6b5a95f25ab2a053d98ee8b9c2d2c \
             --control-plane --certificate-key 8f9c35637c11ef33d5c9aec7808d4a8d7c42f7828dd297d69c2b985bcfb30915
         
         Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
         As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
         "kubeadm init phase upload-certs --upload-certs" to reload certs afterward.
         
         Then you can join any number of worker nodes by running the following on each as root:
         
         kubeadm join 192.168.1.145:12567 --token abcdef.0123456789abcdef \
             --discovery-token-ca-cert-hash sha256:9463bec075db3d70e659c43d578e7658f2e6b5a95f25ab2a053d98ee8b9c2d2c
         
         ```

      4. 安装网络插件

         `kubectl apply -f kube-flannel.yml`

   4. 加入新的控制平面

      ```shell
      kubeadm join 192.168.1.145:12567 --token abcdef.0123456789abcdef \
          --discovery-token-ca-cert-hash sha256:9463bec075db3d70e659c43d578e7658f2e6b5a95f25ab2a053d98ee8b9c2d2c \
          --control-plane --certificate-key 8f9c35637c11ef33d5c9aec7808d4a8d7c42f7828dd297d69c2b985bcfb30915
      
      # 查看节点
      [root@master ~]# kubectl get nodes
      NAME     STATUS   ROLES    AGE     VERSION
      master   Ready    master   10m     v1.17.3
      node1    Ready    master   3m12s   v1.17.3
      
      # 同样的方式加入第三个master节点
      ```

3. 外置`etcd`方案的差异

   - 需要提前为`kubeadm`提供一组高可用的`etcd`集群

   - 将访问`etcd`所需的证书复制到控制平面所在的服务器上

   - 在创建`init-default.yaml`时需要加入`etcd`的访问信息，例如

     ```yaml
     apiServer:
       timeoutForControlPlane: 4m0s
     apiVersion: kubeadm.k8s.io/v1beta2
     certificatesDir: /etc/kubernetes/pki
     clusterName: mycluster
     controllerManager: {}
     dns:
       type: CoreDNS
     etcd:
       local:
         dataDir: /var/lib/etcd
     imageRepository: registry.cn-shenzhen.aliyuncs.com/jzh  # 修改镜像源
     kind: ClusterConfiguration
     kubernetesVersion: v1.17.3
     controlPlaneEndpoint: "192.168.1.145:12567"   # HAProxy提供的代理端口
     etcd:
       external:
         endpoints:
         - https://ETCD_0_IP:2379    
         - https://ETCD_1_IP:2379
         - https://ETCD_2_IP:2379
         caFile: /etc/kubernetes/pki/etcd/ca.crt
         certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
         ketFile: /etc/kubernetes/pki/apiserver-etcd-client.key
     networking:
       dnsDomain: cluster.local
       serviceSubnet: 10.96.0.0/12
       podSubnet: 10.244.0.0/16
     scheduler: {}
     ```

4. 补充说明：

   - `HAProxy`成为新的单点，可能导致整体发生故障，因此在实际工作中需要根据生产环境的具体情况，换用硬件负载均衡器方案，或者用软件进行`VIP，DNS`等相关高可用保障，从而消除对单一`HAProxy`的依赖